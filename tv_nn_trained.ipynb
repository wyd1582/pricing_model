{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural network train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import libraries and packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "#translate from the matlab code\n",
    "import xlrd\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import InputLayer, Input\n",
    "from tensorflow.python.keras.layers import Reshape, MaxPooling2D\n",
    "from tensorflow.python.keras.layers import Conv2D, Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "#translate from the matlab code\n",
    "import xlrd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "NumSKU=49\n",
    "NumVar=13\n",
    "ID=[1138263,1139362,1139363,1141061,1142731,1143640,1144140,\n",
    "        1148001,1148010,1148081,1162466,1162467,1162557,1162558,\n",
    "        1162559,1163152,1163153,1164313,1164961,1164962,1165757,\n",
    "        1166153,1166984,1166998,1167021,1167087,1167847,1167918,\n",
    "        1170236,1170372,1170739,1173299,1174241,1174242,1174243,\n",
    "        1174244,1174275,1174293,1174299,1174313,1174314,1174315,\n",
    "        1174339,1174340,1175687,1175833,1175835,1175950,1177151]\n",
    "\n",
    "#train data\n",
    "new_tv_info = pd.read_excel('C:/Users/wyd15/Downloads/Television.xlsx', sheet_name='new_tv_info') #processed by matlab\n",
    "tv_sim = pd.read_excel('Television.xlsx', sheet_name='Similarity')\n",
    "#tv_sim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24697\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>SalesQuantity</th>\n",
       "      <th>SalesQuantityLag1</th>\n",
       "      <th>SalesQuantityLag7</th>\n",
       "      <th>SalesQuantityLag14</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "      <th>InventoryAvailability</th>\n",
       "      <th>WeekOfYear</th>\n",
       "      <th>...</th>\n",
       "      <th>Var75</th>\n",
       "      <th>Var76</th>\n",
       "      <th>Var77</th>\n",
       "      <th>Var78</th>\n",
       "      <th>Var79</th>\n",
       "      <th>Var80</th>\n",
       "      <th>Var81</th>\n",
       "      <th>Var82</th>\n",
       "      <th>Var83</th>\n",
       "      <th>Var84</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2626.27000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1779.66000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2329.659900</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2533.055050</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2541.530000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1753.387550</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2651.69398</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1880.51008</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2365.113317</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2555.651667</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2794.132285</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1842.372600</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2774.57500</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1851.69608</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2355.083980</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2725.285017</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2835.449192</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1789.243869</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2795.76000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1876.27505</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2468.837450</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2965.260000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2721.716275</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1905.935100</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2795.76000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1876.27505</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2468.837450</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2626.273333</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3473.720000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1905.925200</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID       Date  SalesQuantity  SalesQuantityLag1  SalesQuantityLag7  \\\n",
       "0   1 2016-01-01              2                  1                  1   \n",
       "1   1 2016-01-02              6                  2                  1   \n",
       "2   1 2016-01-03              5                  6                  1   \n",
       "3   1 2016-01-04              6                  5                  1   \n",
       "4   1 2016-01-05              2                  6                  1   \n",
       "\n",
       "   SalesQuantityLag14       Price  Discount  InventoryAvailability  \\\n",
       "0                   1  2626.27000      1.01                   0.93   \n",
       "1                   1  2651.69398      1.01                   0.93   \n",
       "2                   1  2774.57500      1.01                   0.93   \n",
       "3                   1  2795.76000      1.01                   0.93   \n",
       "4                   1  2795.76000      1.01                   0.93   \n",
       "\n",
       "   WeekOfYear  ...         Var75  Var76        Var77  Var78        Var79  \\\n",
       "0           1  ...    1779.66000   1.01  2329.659900   1.01  2533.055050   \n",
       "1           1  ...    1880.51008   1.01  2365.113317   1.01  2555.651667   \n",
       "2           1  ...    1851.69608   1.01  2355.083980   1.01  2725.285017   \n",
       "3           2  ...    1876.27505   1.01  2468.837450   1.01  2965.260000   \n",
       "4           2  ...    1876.27505   1.01  2468.837450   1.01  2626.273333   \n",
       "\n",
       "   Var80        Var81  Var82        Var83  Var84  \n",
       "0   1.01  2541.530000   1.01  1753.387550   1.01  \n",
       "1   1.01  2794.132285   1.01  1842.372600   1.01  \n",
       "2   1.01  2835.449192   1.01  1789.243869   1.01  \n",
       "3   1.01  2721.716275   1.01  1905.935100   1.01  \n",
       "4   1.01  3473.720000   1.01  1905.925200   1.01  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(new_tv_info))\n",
    "new_tv_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, train_loss: 22379.767578, test_loss: 145694.078125\n",
      "iter: 100, train_loss: 261.362335, test_loss: 67290.007812\n",
      "iter: 200, train_loss: 255.445541, test_loss: 4472.424805\n",
      "iter: 300, train_loss: 44.787533, test_loss: 516.710327\n",
      "iter: 400, train_loss: 1.891456, test_loss: 23.775335\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_placeholder(input1_dim=18, input2_dim=61):\n",
    "    return tf.placeholder(shape=(None, input1_dim), dtype=tf.float32), \\\n",
    "           tf.placeholder(shape=(None, input2_dim), dtype=tf.float32), \\\n",
    "           tf.placeholder(shape=(None, ), dtype=tf.float32)\n",
    "\n",
    "\n",
    "def get_output(input1, input2):\n",
    "     \n",
    "    input1_dim = input1.get_shape()[1]\n",
    "    input2_dim = input2.get_shape()[1]\n",
    "    W1_1 = tf.get_variable('W1_1', shape=(input1_dim, input1_dim+input2_dim))\n",
    "    W1_2 = tf.get_variable('W1_2', shape=(input2_dim, input1_dim+input2_dim))\n",
    "    b1 = tf.get_variable('b1', shape=(1, input1_dim+input2_dim))\n",
    "    \n",
    "    o1_1 = tf.matmul(input1, W1_1)\n",
    "    o1_2 = tf.matmul(input2, W1_2)\n",
    "    o1 = o1_1 + o1_2 + b1\n",
    "    o1 = tf.nn.sigmoid(o1)\n",
    "\n",
    "    W2_1 = tf.get_variable('W2_1', shape=(input2_dim, 1))\n",
    "    W2_2 = tf.get_variable('W2_2', shape=(input1_dim+input2_dim, 1))\n",
    "    b2 = tf.get_variable('b2', shape=(1, 1))\n",
    "    o2_1 = tf.matmul(input2, W2_1)\n",
    "    o2_2 = tf.matmul(o1, W2_2)\n",
    "    o2 = o2_1 + o2_2 + b2\n",
    "    o2 = tf.reshape(o2, (-1, ))\n",
    "    \n",
    "    saver = tf.train.Saver([W1_1, W1_2, b1, W2_1, W2_2, b2])\n",
    "    Vars=[W1_1, W1_2, b1, W2_1, W2_2, b2]\n",
    "    \n",
    "    return o2, saver, Vars\n",
    "\n",
    "\n",
    "def get_loss(labels, predictions):\n",
    "    return tf.losses.mean_squared_error(labels, predictions)\n",
    "\n",
    "\n",
    "def main():\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    batch_size = 50\n",
    "    learning_rate = 0.11\n",
    "    tf.reset_default_graph()\n",
    "    x1, x2, y = get_placeholder(18, 61)\n",
    "    output, saver, Vars = get_output(x1, x2)\n",
    "\n",
    "    loss = get_loss(y, output)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    #train-test_splie\n",
    "    msk = np.random.rand(len(new_tv_info)) < 0.85\n",
    "    tv_train = new_tv_info[msk]\n",
    "    tv_test = new_tv_info[~msk]\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for step in range(493):\n",
    "            x1_train = tv_train.iloc[step:step+batch_size, 66:84].values \n",
    "            x2_train = np.concatenate((tv_train.iloc[step:step+batch_size, 3:11].values, tv_train.iloc[step:step+batch_size, 14:67].values), 1) \n",
    "            y_train = tv_train.iloc[step:step+batch_size, 2].values \n",
    "            \n",
    "            x1_test = tv_test.iloc[step:step+batch_size, 66:84].values \n",
    "            x2_test = np.concatenate((tv_test.iloc[step:step+batch_size, 3:11].values, tv_test.iloc[step:step+batch_size, 14:67].values), 1) \n",
    "            y_test = tv_test.iloc[step:step+batch_size, 2].values \n",
    "            \n",
    "            _train_loss, _ = sess.run([loss, train_op],\n",
    "                                   feed_dict={x1: x1_train,\n",
    "                                              x2: x2_train,\n",
    "                                              y: y_train})\n",
    "            \n",
    "            _test_loss= sess.run([loss],\n",
    "                                   feed_dict={x1: x1_test,\n",
    "                                              x2: x2_test,\n",
    "                                              y: y_test})\n",
    "            \n",
    "            #results, _ = sess.run([output]) #print parameters, W1_1 etc..\n",
    "            \n",
    "            if step % 100==0:\n",
    "                saver.save(sess, 'C:/Users/wyd15/Desktop/tv_model/tv_modelslack.ckpt', global_step=step)\n",
    "                print(\"iter: %d, train_loss: %f, test_loss: %f\"%(step, _train_loss,  _test_loss[0]))\n",
    "                #print('output:', )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# original tv demand neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0, train_loss: 484165.875000, test_loss: 13285.414062\n",
      "test prediction parameters results: [array([ 142.6806488 ,  144.60281372,  144.73997498,  134.95324707,\n",
      "        138.21200562,  134.62776184,  140.89408875,  134.02508545,\n",
      "        134.77386475,  140.45220947,  138.36047363,  146.27722168,\n",
      "        130.59552002,  130.54397583,  151.14276123,  131.98068237,\n",
      "        131.17425537,  134.09301758,  135.7194519 ,  115.26747894,\n",
      "        137.05897522,  123.21807098,  121.61158752,  121.65653992,\n",
      "        120.83131409,  112.87034607,  124.26582336,  117.22241974,\n",
      "        118.47533417,  122.93070221,  122.1912384 ,  120.49742126,\n",
      "        114.342453  ,  113.62680054,  121.43595886,  103.77321625,\n",
      "        114.78121185,  114.32935333,  108.43563843,  108.81217194,\n",
      "        109.18531799,   97.17289734,  100.35605621,   96.27137756,\n",
      "         97.4725647 ,   84.61821747,   84.87034607,   85.0085144 ,\n",
      "         84.57138062,   84.58546448], dtype=float32)]\n",
      "iter:100, train_loss: 79.309921, test_loss: 1767.102295\n",
      "test prediction parameters results: [array([ -6.12736607,  -6.19863701,  -6.26893425,  -5.72890806,\n",
      "        -5.63114738, -68.52324677, -87.70921326, -71.61974335,\n",
      "       -81.95714569, -66.85129547, -66.32038116, -66.63104248,\n",
      "       -67.57054138, -67.65065765, -67.97412109, -68.13474274,\n",
      "       -68.84365845, -68.23690796, -69.09172058, -69.68632507,\n",
      "       -69.51461792,   2.54343581,   5.19236946,   6.06464529,\n",
      "         7.37892008,   0.21512407,   6.72181892,   3.82938981,\n",
      "        20.01004982,  16.00852394,   3.12199211,  15.53723717,\n",
      "         7.11093664,   2.80306935,   4.41062164,  -3.64349556,\n",
      "        -9.25739193,   3.45452332,   5.68409109,   0.73576707,\n",
      "        -5.04203463,  -7.97847033, -10.90910721, -22.96694565,\n",
      "       -23.96250916,  -6.16617584,   2.93668389, -20.76041412,\n",
      "       -19.79692841, -21.00598907], dtype=float32)]\n",
      "iter:200, train_loss: 16.822750, test_loss: 378.319611\n",
      "test prediction parameters results: [array([-19.94377136, -20.84614944, -21.48228073, -20.52012253,\n",
      "       -21.53390503, -21.80030823, -21.48220062, -22.05724716,\n",
      "       -22.35941696, -22.16995621, -22.57122803, -22.7543869 ,\n",
      "       -22.88925552, -23.20728302,  -1.55791903, -40.09410095,\n",
      "       -40.96390915, -41.12695312, -41.75186539,   4.24955273,\n",
      "         2.966537  ,   2.91490889,   2.51363325,  -4.4624486 ,\n",
      "        -3.64903951,  -1.54015017,  13.97749996,  13.64863873,\n",
      "        13.77317333,  12.75938702,  13.45514202,   4.51191616,\n",
      "         5.15597439,   4.14218807,   4.11894035,   3.82416463,\n",
      "         3.87586141,   3.23973036,   3.10486555,   2.99865675,\n",
      "         2.60367537,   2.92178297,  24.80954742,  32.81396103,\n",
      "        20.98145676,  24.47816086,  29.59928131,  18.7815361 ,\n",
      "        28.42969894,  19.83240891], dtype=float32)]\n",
      "iter:300, train_loss: 15.186463, test_loss: 209.216156\n",
      "test prediction parameters results: [array([  7.76155949,   7.61735439,   7.86719227,   7.63299656,\n",
      "         7.63162136,  10.85046864,   9.95629787, -15.25336838,\n",
      "       -14.91722202, -13.4201231 , -12.85601807, -16.25438499,\n",
      "       -16.49735641, -16.12915993, -14.69331932, -14.90207386,\n",
      "       -14.65223598, -15.08317184, -15.90360355, -18.88882828,\n",
      "       -18.97849655, -16.16527557, -15.49377251, -16.28229332,\n",
      "       -16.63427544, -15.96489716, -16.86944199, -16.94680786,\n",
      "       -16.7832489 ,  -1.37702703,  -0.83650643,  -1.84846306,\n",
      "        -2.3024435 ,  -1.07207727,  -2.42022943, -16.06371117,\n",
      "       -14.97912979, -15.76766777, -16.10887337, -15.33248711,\n",
      "         1.07151365,   0.67201799,   1.84789896,   3.38842821,\n",
      "         3.02679253, -19.76167297, -20.55382156, -20.24410248,\n",
      "       -20.40941238, -19.41837883], dtype=float32)]\n",
      "iter:400, train_loss: 0.751397, test_loss: 1.344561\n",
      "test prediction parameters results: [array([-0.37699556, -0.40110826, -1.10520291, -1.25955391, -1.2034483 ,\n",
      "       -1.24485493, -0.43749356, -1.20199156, -1.24399161, -1.57997465,\n",
      "       -1.33229923,  2.17117071,  2.12911391,  1.99294734,  1.88838053,\n",
      "        1.68596506,  1.91261816,  0.65734291,  0.68867016,  1.00491214,\n",
      "        0.89158762,  0.50282705,  0.9927876 ,  0.91333711,  0.6083765 ,\n",
      "        0.5580132 ,  0.9408927 ,  0.95856643,  0.64923656,  0.81740963,\n",
      "        1.04192257,  1.17387056,  4.22780991,  0.91617489,  1.16806173,\n",
      "        0.99085104,  0.74286199,  1.21516633,  1.04375565,  0.92097044,\n",
      "        0.67203045,  0.42190087,  0.56497538,  0.69555473,  1.35868454,\n",
      "        0.53506303,  0.27059269,  0.49093103,  0.44944656,  0.36402273], dtype=float32)]\n",
      "last test prediction parameters results: 492 0.267434 2.01754 [array([ 0.61402762, -0.65702546,  2.3991394 ,  3.08483362, -0.49968064,\n",
      "       -0.31641352, -0.80965841, -0.55702174, -0.68362916, -0.97832239,\n",
      "       -0.50080073, -1.0538429 ,  2.24409246,  1.72188675,  0.85900962,\n",
      "        1.73871338,  1.43203795,  0.68387604,  0.92696357,  0.84599221,\n",
      "        0.70094919,  0.31711507,  0.51296687,  1.49841392,  0.58016479,\n",
      "        0.72272789,  1.01515281,  4.10374737,  0.3129766 ,  0.13534689,\n",
      "        0.64635158,  0.68633759, -0.09350264, -0.13347018,  0.08393633,\n",
      "       -0.34442079, -0.59547102, -0.36270654, -0.66307771, -0.7595278 ,\n",
      "       -0.64087355, -0.55917418, -0.89855874, -0.82646382, -0.59369934,\n",
      "       -0.70045555, -0.95800745, -0.16539919, -0.81521785, -0.92238224], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# tensorflow nn\n",
    "#unfolded version\n",
    "#reference code: https://github.com/yl3829/Spring2018-Project5-grp5/blob/master/lib/model_tb.py  \n",
    "\n",
    "# D = tf.placeholder(dytpe=np.floate32)\n",
    "# w1_1 = tf.Variable((18,79),name='W1_1')\n",
    "# w1_2 = tf.Variable((61,79),name='W1_2')\n",
    "# b1 = tf.Variable((79,),name='b1')\n",
    "# param = [w1_1,w1_2,b1]\n",
    "tf.reset_default_graph()\n",
    "x1=tf.placeholder(shape=(None, 18), dtype=tf.float32)\n",
    "x2=tf.placeholder(shape=(None, 61), dtype=tf.float32)\n",
    "y =tf.placeholder(shape=(None, ), dtype=tf.float32)\n",
    "\n",
    "#\n",
    "W1_1 = tf.get_variable('W1_1', shape=(18, 79))\n",
    "W1_2 = tf.get_variable('W1_2', shape=(61, 79))\n",
    "b1 = tf.get_variable('b1', shape=(1, 79))\n",
    "    \n",
    "o1_1 = tf.matmul(x1, W1_1)\n",
    "o1_2 = tf.matmul(x2, W1_2)\n",
    "o1 = o1_1 + o1_2 + b1\n",
    "o1 = tf.nn.sigmoid(o1)\n",
    "\n",
    "W2_1 = tf.get_variable('W2_1', shape=(61, 1))\n",
    "W2_2 = tf.get_variable('W2_2', shape=(79, 1))\n",
    "b2 = tf.get_variable('b2', shape=(1, 1))\n",
    "\n",
    "o2_1 = tf.matmul(x2, W2_1)\n",
    "o2_2 = tf.matmul(o1, W2_2)\n",
    "o2 = o2_1 + o2_2 + b2\n",
    "o2 = tf.reshape(o2, (-1, ))\n",
    "\n",
    "saver = tf.train.Saver([W1_1, W1_2, b1, W2_1, W2_2, b2])\n",
    "#\n",
    "\n",
    "loss = tf.losses.mean_squared_error(y, o2)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.11)\n",
    "train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "# layer1_out  = tf.nn.sigmoid(tf.matmul(input1,w1_1) + tf.matmul(input2,w1_2) + b1) # tanh would be better\n",
    "# # example to minimize layer1 out put\n",
    "# losses = layer2_out - D\n",
    "# loss = tf.reduce_mean(losses)\n",
    "# trainer = tf.train.AdamOptimizer()\n",
    "# gradients = trainer.compute_gradients(self.loss)\n",
    "# optimizer = trainer.apply_gradients(gradients)\n",
    "\n",
    "#train-test_splie\n",
    "msk = np.random.rand(len(new_tv_info)) < 0.85\n",
    "tv_train = new_tv_info[msk]\n",
    "tv_test = new_tv_info[~msk]\n",
    "batch_size=50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(493):\n",
    "        x1_train = tv_train.iloc[step:step+batch_size, 66:84].values \n",
    "        x2_train = np.concatenate((tv_train.iloc[step:step+batch_size, 3:11].values, tv_train.iloc[step:step+batch_size, 14:67].values), 1) \n",
    "        y_train = tv_train.iloc[step:step+batch_size, 2].values \n",
    "            \n",
    "        x1_test = tv_test.iloc[step:step+batch_size, 66:84].values \n",
    "        x2_test = np.concatenate((tv_test.iloc[step:step+batch_size, 3:11].values, tv_test.iloc[step:step+batch_size, 14:67].values), 1) \n",
    "        y_test = tv_test.iloc[step:step+batch_size, 2].values \n",
    "            \n",
    "        _train_loss, _ = sess.run([loss, train_op],feed_dict={x1: x1_train,\n",
    "                                              x2: x2_train,\n",
    "                                              y: y_train})\n",
    "            \n",
    "        _test_loss= sess.run([loss],feed_dict={x1: x1_test,\n",
    "                                              x2: x2_test,\n",
    "                                              y: y_test})\n",
    "        # predict\n",
    "        D_predict = sess.run([o2], feed_dict={x1:x1_test,x2:x2_test})\n",
    "        para_pred = sess.run([W1_1, W1_2, b1, W2_1, W2_2, b2], feed_dict={x1:x1_test,x2:x2_test})\n",
    "        #results, _ = sess.run([output]) #print parameters, W1_1 etc..\n",
    "            \n",
    "        if step % 100==0:\n",
    "            saver.save(sess, 'C:/Users/wyd15/Desktop/tv_model/tv_modelslack.ckpt', global_step=step)\n",
    "            print(\"iter:%d, train_loss: %f, test_loss: %f\"%(step, _train_loss,  _test_loss[0]))\n",
    "            print('test prediction parameters results:', D_predict)\n",
    "            #print('predicted parameters:', para_pred)\n",
    "    print('last test prediction parameters results:', step, _train_loss,  _test_loss[0], D_predict)\n",
    "    paras=para_pred\n",
    "                #print('output:', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# printing tv nn trained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ===== printing paramter: W1_1 ===== \n",
      "[[ -5.18386364e-02   8.38602930e-02  -1.80666959e+00   1.10418415e+00\n",
      "   -2.07713336e-01  -2.21424723e+00  -2.44942665e-01  -4.10063267e-02\n",
      "   -9.75947380e-02   8.58362764e-02  -1.22434035e-01   1.89107850e-01\n",
      "   -9.15461332e-02   2.25905493e-01   6.66529477e-01   1.60307109e+00\n",
      "    1.73369154e-01  -1.64197087e-02  -8.60521793e-02  -6.75001621e-01\n",
      "    9.11085159e-02  -1.83954120e+00   1.59876630e-01   4.77414876e-02\n",
      "    5.18515527e-01  -2.44292289e-01   1.11651465e-01   4.39683169e-01\n",
      "   -1.10453701e+00  -2.40049273e-01  -1.41516089e-01  -1.58604667e-01\n",
      "    7.61337578e-03  -2.41686210e-01   2.01219583e+00   1.25279856e+00\n",
      "    1.37263775e+00  -1.51915237e-01  -1.34219289e-01   1.71839669e-01\n",
      "   -1.11546290e+00   2.30179280e-02   6.92257136e-02  -6.31567359e-01\n",
      "   -1.05009302e-01   1.65775701e-01   1.88918263e-02  -5.79876363e-01\n",
      "   -5.75691760e-02   1.97266951e-01   7.24060088e-02   1.63395837e-01\n",
      "    1.55686259e+00  -2.38423645e-01  -1.35819423e+00   1.95601910e-01\n",
      "   -1.82669222e+00  -8.28132629e-01  -1.62510306e-01  -1.69291466e-01\n",
      "   -7.20560551e-01   1.43132582e-01  -9.48099792e-03  -1.45569563e-01\n",
      "   -5.36408782e-01  -1.91007242e-01  -4.99776155e-02   4.30130512e-02\n",
      "   -6.59455955e-02  -2.92392224e-02  -2.41923153e-01  -3.53250653e-02\n",
      "   -1.49864882e-01  -1.80010736e-01  -8.42782795e-01   1.74227333e+00\n",
      "   -6.99632764e-01   2.20817477e-02  -1.89988762e-01]\n",
      " [  4.75261062e-02  -1.66099340e-01  -1.62270343e+00   8.14575434e-01\n",
      "    1.35088101e-01  -2.22891617e+00   8.11176449e-02   1.11696020e-01\n",
      "   -2.44883835e-01   2.41903827e-01  -1.73768163e-01  -1.60639316e-01\n",
      "    2.23720536e-01  -2.43313313e-01   7.31054902e-01   1.70356357e+00\n",
      "    6.88428730e-02   3.95533293e-02  -2.84815282e-02  -7.09432185e-01\n",
      "    1.80831954e-01  -2.11497211e+00   9.80340093e-02  -2.83518434e-02\n",
      "    4.34382111e-01  -1.54493392e-01  -1.50629684e-01   8.07184935e-01\n",
      "   -1.05235326e+00   2.09363326e-01   2.41584823e-01  -2.45707884e-01\n",
      "   -2.47410342e-01   1.78461805e-01   1.69107687e+00   1.26406908e+00\n",
      "    6.39607966e-01   7.59036839e-03  -1.19368047e-01  -1.25868678e-01\n",
      "   -8.94037485e-01  -2.05248564e-01  -1.94034040e-01  -6.83934629e-01\n",
      "    1.39669433e-01  -1.11737520e-01   9.82955098e-04  -8.78154755e-01\n",
      "   -1.13860995e-01  -1.51901066e-01   9.62795466e-02  -1.41303748e-01\n",
      "    2.38925874e-01  -1.55782446e-01   1.66690961e-01  -2.68925745e-02\n",
      "   -1.72052443e+00  -1.86239839e-01   2.33779058e-01  -2.31603533e-01\n",
      "   -4.81136769e-01  -1.77401751e-01  -1.47065073e-02  -1.11391574e-01\n",
      "   -4.84182686e-01   2.11215764e-02  -1.76992655e-01  -1.83358625e-01\n",
      "    1.49927035e-01  -3.05838883e-02   8.93354863e-02  -1.97405398e-01\n",
      "    1.77753672e-01   1.94017425e-01  -5.66779256e-01   1.60119522e+00\n",
      "   -3.62416834e-01  -5.21648824e-02  -2.87771225e-02]\n",
      " [ -7.09335059e-02  -4.89815474e-02  -1.85810447e+00   8.50102365e-01\n",
      "    6.08932823e-02  -2.16467881e+00   2.46594399e-02  -2.60691345e-03\n",
      "    1.87480286e-01  -6.19792491e-02  -2.75733322e-02  -1.79137304e-01\n",
      "   -1.61207914e-01   1.62400454e-02   6.47968471e-01   1.51713169e+00\n",
      "    5.15749902e-02  -1.69026703e-02  -3.92351449e-02  -5.24235249e-01\n",
      "    9.00416523e-02  -2.00623298e+00  -1.82060152e-02  -2.46113122e-01\n",
      "    7.65000880e-01   1.39379159e-01   7.36357719e-02   5.54332197e-01\n",
      "   -9.92928147e-01  -1.07530192e-01   2.14681998e-01  -1.84022993e-01\n",
      "   -1.76816583e-02  -1.22034490e-01   2.24063087e+00   1.26005507e+00\n",
      "    1.43969321e+00   1.76693991e-01   7.29348809e-02  -1.20888054e-01\n",
      "   -9.17917013e-01   1.71378210e-01  -2.30830014e-01  -5.08828402e-01\n",
      "    2.90425867e-02  -1.23123184e-01  -1.65952891e-02  -8.59730899e-01\n",
      "    3.36078852e-02  -1.03268474e-01   2.29093686e-01  -2.21792489e-01\n",
      "    1.72839820e+00   2.09094137e-02  -9.27967429e-01   1.62230238e-01\n",
      "   -1.98498213e+00  -8.85367155e-01  -5.29275537e-02   2.37014040e-01\n",
      "   -5.98300338e-01  -2.42071047e-01  -1.98480561e-01  -2.10119188e-01\n",
      "   -6.45803213e-01  -9.51927602e-02   1.02940276e-01  -2.44313642e-01\n",
      "   -2.29706630e-01  -2.21700519e-02  -1.81124091e-01  -8.84868503e-02\n",
      "   -2.16219380e-01   1.45880684e-01  -7.62388706e-01   1.73423815e+00\n",
      "   -7.97317505e-01   7.78187960e-02   1.10260442e-01]\n",
      " [ -6.79532588e-02   1.44084945e-01  -1.63174832e+00   8.39427114e-01\n",
      "    1.27555594e-01  -2.08796358e+00  -1.85181767e-01  -4.62614298e-02\n",
      "    4.57925051e-02  -1.95431948e-01  -2.16575399e-01  -1.88258126e-01\n",
      "    1.46450177e-01   1.95098370e-02   5.55936277e-01   1.70965171e+00\n",
      "   -7.18762577e-02   1.33624777e-01  -5.17742932e-02  -5.12242913e-01\n",
      "   -1.62979454e-01  -2.22902679e+00  -3.27228010e-03   1.52563527e-01\n",
      "    5.43141961e-01  -6.42862320e-02  -2.08795816e-02   8.28075409e-01\n",
      "   -9.16967630e-01   2.45849416e-01   2.21562728e-01  -2.44143754e-02\n",
      "   -4.41634655e-02   1.98906079e-01   2.06254840e+00   1.28716218e+00\n",
      "    5.24954498e-01  -1.19437128e-01  -5.26766777e-02  -1.80784196e-01\n",
      "   -7.53255427e-01   2.21623793e-01  -7.30183721e-02  -9.05862868e-01\n",
      "   -1.42903462e-01   1.46338195e-02   6.10619336e-02  -4.77338195e-01\n",
      "   -1.75838098e-01   4.87738997e-02   6.17887080e-03  -1.18462458e-01\n",
      "    1.03006393e-01  -1.54675022e-01  -1.87681228e-01  -2.27156296e-01\n",
      "   -1.93689394e+00  -1.18957438e-01  -2.22163275e-01   1.03706315e-01\n",
      "   -3.34566325e-01  -1.02137819e-01   1.47022679e-01   2.02596232e-01\n",
      "   -6.40702844e-01  -2.41041332e-02  -1.07927889e-01  -9.02961642e-02\n",
      "    2.41013005e-01  -1.68266103e-01  -2.22852781e-01  -7.85470754e-02\n",
      "    1.09399408e-02  -1.46280468e-01  -8.15234661e-01   1.54367173e+00\n",
      "   -5.99364102e-01   1.15132436e-01  -8.64161998e-02]\n",
      " [ -3.64337862e-02   2.30220810e-01  -2.06719613e+00   9.42984402e-01\n",
      "   -2.41064847e-01  -2.21879649e+00   1.25607848e-03   2.14568809e-01\n",
      "   -2.27201656e-01   1.42946437e-01   1.55540854e-02   1.24765933e-03\n",
      "   -9.08254981e-02   2.26899073e-01   6.35828793e-01   1.25870061e+00\n",
      "    1.32008955e-01   5.85395545e-02   1.18577674e-01  -4.80527729e-01\n",
      "   -1.95784882e-01  -2.18482876e+00  -1.10090077e-02   1.36014566e-01\n",
      "    7.23742723e-01  -2.09124371e-01   8.70843083e-02   6.75188363e-01\n",
      "   -8.53405833e-01  -3.43772024e-02   1.12546846e-01  -2.36655176e-02\n",
      "    2.23850325e-01  -3.67192477e-02   1.81853902e+00   1.31511664e+00\n",
      "    1.56006026e+00   9.67252702e-02  -2.44006068e-01   4.32904512e-02\n",
      "   -8.44297111e-01  -2.45639339e-01   1.21713296e-01  -6.04383886e-01\n",
      "    2.03105405e-01   1.44326761e-01   2.25780755e-02  -7.95800865e-01\n",
      "   -1.05419636e-01   4.42437977e-02  -1.91095471e-03  -6.35412931e-02\n",
      "    1.47348797e+00   1.78980157e-01  -1.32074618e+00   4.69074905e-01\n",
      "   -2.07939148e+00  -1.11151683e+00   1.66596904e-01  -4.77486551e-02\n",
      "   -4.19073820e-01  -1.03002593e-01  -3.33203077e-02  -2.91680545e-02\n",
      "   -6.65072858e-01  -2.31905237e-01  -1.57589093e-01   1.69197068e-01\n",
      "   -2.38695875e-01  -1.67343378e-01   6.49816245e-02  -2.83678472e-02\n",
      "   -9.57987607e-02   4.69008833e-02  -6.77790582e-01   1.61072922e+00\n",
      "   -5.16393960e-01   8.56528729e-02   5.92367798e-02]\n",
      " [ -3.32940370e-02  -8.77732038e-02  -1.34891331e+00   1.03656733e+00\n",
      "   -3.31006050e-02  -2.03528261e+00   7.09625036e-02   1.12989798e-01\n",
      "   -5.78460395e-02   2.79917866e-02   1.27536461e-01   6.09642714e-02\n",
      "    2.07576737e-01   9.67193991e-02   8.09579730e-01   1.70707774e+00\n",
      "   -1.82777405e-01  -2.18801036e-01   8.96728486e-02  -5.50443769e-01\n",
      "    1.82440117e-01  -2.21094084e+00  -6.87563717e-02   6.30321652e-02\n",
      "    7.00363696e-01  -2.13955969e-01   1.60395652e-02   4.68071759e-01\n",
      "   -1.04389703e+00   7.45699853e-02  -1.50075614e-01  -1.51557922e-02\n",
      "   -2.23010689e-01  -2.29895443e-01   2.00229430e+00   1.08324337e+00\n",
      "    4.81453568e-01   8.70278031e-02   4.15267199e-02  -1.83103532e-01\n",
      "   -4.75372732e-01   1.15862951e-01   1.24791250e-01  -9.04057801e-01\n",
      "   -8.03501606e-02  -2.89831161e-02  -1.41947657e-01  -8.58401179e-01\n",
      "   -1.75721943e-02  -3.10172290e-02   5.72696179e-02  -1.29673913e-01\n",
      "    3.36190432e-01  -9.94096994e-02   4.98263612e-02   2.24901527e-01\n",
      "   -1.85128391e+00  -5.24277687e-01   1.35162339e-01   2.46945158e-01\n",
      "   -7.78250694e-01   1.74219176e-01  -1.09089926e-01   9.45279151e-02\n",
      "   -8.29023600e-01  -1.51604474e-01   2.42669836e-01   9.26271081e-04\n",
      "    1.12304702e-01  -2.14995980e-01  -2.08566323e-01  -1.30554646e-01\n",
      "   -1.20500132e-01   2.60960907e-02  -5.14358222e-01   1.63578582e+00\n",
      "   -4.52256918e-01  -1.77229315e-01  -1.59044772e-01]\n",
      " [  3.51860672e-02  -5.85615635e-04  -2.19011474e+00   9.53165591e-01\n",
      "    1.51403800e-01  -2.01176548e+00   1.92378297e-01  -1.41633749e-01\n",
      "   -1.48670226e-01   1.58263192e-01  -1.16485104e-01  -2.95182616e-02\n",
      "   -3.33898664e-02   2.15562984e-01   5.28965592e-01   1.62461245e+00\n",
      "    1.30194947e-01   3.38449627e-02   8.81187618e-03  -6.42026603e-01\n",
      "    5.68518788e-02  -1.80125415e+00   2.38756970e-01   1.31769821e-01\n",
      "    7.08561599e-01  -8.71513039e-02   2.12212875e-01   8.86287332e-01\n",
      "   -9.92530167e-01  -7.71319568e-02  -2.45448202e-02   1.96335152e-01\n",
      "   -1.61483049e-01  -2.44419307e-01   2.22824693e+00   1.31803000e+00\n",
      "    1.16529822e+00   1.36491254e-01   1.87366083e-01  -1.93059906e-01\n",
      "   -1.15794194e+00  -4.85797524e-02   5.14619201e-02  -7.78150141e-01\n",
      "   -8.07000697e-02   1.89500913e-01  -2.27517173e-01  -4.43471164e-01\n",
      "    1.80388168e-01   1.69310853e-01   4.76205200e-02   1.26035646e-01\n",
      "    1.49136996e+00   1.78106442e-01  -1.08062530e+00   2.29853421e-01\n",
      "   -2.26733208e+00  -1.09758782e+00  -2.69104540e-02  -2.08212852e-01\n",
      "   -5.74113488e-01   3.14008743e-02   1.06181428e-01  -5.60492277e-02\n",
      "   -4.40930754e-01   2.05381200e-01  -1.08309701e-01  -1.02142796e-01\n",
      "   -1.82505876e-01   2.09358707e-01   1.33446053e-01   1.12147197e-01\n",
      "    8.14843327e-02   2.03479379e-02  -4.15791929e-01   1.62702024e+00\n",
      "   -6.95570111e-01   1.69530466e-01   1.33172944e-01]\n",
      " [ -1.40077144e-01   1.83479175e-01  -1.27354217e+00   6.58358872e-01\n",
      "    2.37569854e-01  -1.84972835e+00   6.96360916e-02   1.29763976e-01\n",
      "    2.94163376e-02   1.48207411e-01  -1.52740359e-01  -1.76988393e-01\n",
      "    1.55985937e-01   1.17209420e-01   8.13207507e-01   1.65722454e+00\n",
      "    1.90510005e-02  -2.78385729e-02  -2.25930750e-01  -5.82061112e-01\n",
      "    2.86381394e-02  -2.16086578e+00   7.73195773e-02   1.55605689e-01\n",
      "    5.74661791e-01  -9.45667624e-02  -1.36176258e-01   4.51161146e-01\n",
      "   -9.53680038e-01   3.31401825e-05   2.29515925e-01   1.34045973e-01\n",
      "   -4.74397093e-02   2.39216372e-01   2.07104874e+00   1.26363170e+00\n",
      "    7.03872263e-01   2.76446342e-04   3.77633423e-02  -4.33455855e-02\n",
      "   -5.81193805e-01  -1.30439132e-01  -2.31371149e-01  -5.65853357e-01\n",
      "   -1.08485162e-01  -4.95529175e-03   1.80673197e-01  -7.49220550e-01\n",
      "    1.54105410e-01  -1.03564247e-01   1.80181935e-01  -2.01451570e-01\n",
      "    9.27629620e-02  -1.66506469e-01   2.42069140e-01   9.89028215e-02\n",
      "   -1.80411243e+00  -2.22622335e-01   3.98844928e-02  -1.44658834e-02\n",
      "   -6.01723433e-01  -2.17776805e-01  -1.81618929e-02  -2.07472891e-01\n",
      "   -6.92706287e-01  -1.51830971e-01  -2.39548504e-01  -8.15086365e-02\n",
      "    1.59268841e-01  -9.94710624e-02   3.96601707e-02   2.08210364e-01\n",
      "    1.87694505e-01   1.17645308e-01  -7.82605588e-01   1.68060541e+00\n",
      "   -4.19068575e-01   4.68928963e-02   2.18483254e-01]\n",
      " [  2.15512976e-01   2.30413094e-01  -2.04809427e+00   1.08039570e+00\n",
      "    1.26929581e-03  -2.01812100e+00   1.79066852e-01   1.17763534e-01\n",
      "   -4.11364436e-03  -6.15310818e-02  -8.12925100e-02   1.30061820e-01\n",
      "    2.63513476e-02  -4.64119911e-02   4.23572570e-01   1.71835220e+00\n",
      "   -8.00794214e-02  -1.10869765e-01  -2.18360052e-01  -8.83885086e-01\n",
      "    2.26127908e-01  -2.08073187e+00  -4.15455103e-02   4.17033583e-02\n",
      "    9.07200515e-01   9.00696367e-02   8.35295767e-02   8.50524187e-01\n",
      "   -7.68957973e-01  -2.37232968e-01  -1.45013824e-01   1.91441938e-01\n",
      "   -7.56577253e-02  -2.31343359e-02   1.85550141e+00   1.46218514e+00\n",
      "    1.35782599e+00   1.28336057e-01  -8.93744528e-02   1.38079971e-02\n",
      "   -7.27418840e-01  -2.34791204e-01   9.11631435e-02  -5.46714842e-01\n",
      "   -9.26025659e-02  -1.46593958e-01  -1.62260979e-02  -5.56802571e-01\n",
      "    1.58071473e-01  -2.01842919e-01  -1.70629829e-01   1.09285250e-01\n",
      "    1.44911528e+00   2.28978053e-01  -1.14832962e+00   2.21861362e-01\n",
      "   -1.93732870e+00  -7.34354913e-01  -1.41474709e-01   1.43473595e-02\n",
      "   -7.09084153e-01  -1.75481901e-01   2.18061790e-01  -2.78529227e-02\n",
      "   -7.26711869e-01   1.48603633e-01   2.24256068e-02  -1.30810320e-01\n",
      "   -1.59451067e-01  -4.35122102e-02   1.03839323e-01  -2.11483240e-01\n",
      "    9.69518721e-03   1.20271012e-01  -5.60124695e-01   1.76532328e+00\n",
      "   -3.48371416e-01   1.02607802e-01  -2.23492652e-01]\n",
      " [  1.62388369e-01  -4.80871201e-02  -1.49669731e+00   6.84563637e-01\n",
      "    1.20001689e-01  -2.26907849e+00  -9.68962908e-04   1.89888611e-01\n",
      "    2.13197336e-01  -1.13324940e-01  -8.11114162e-02   2.06105456e-01\n",
      "    1.90888241e-01   6.02103025e-02   6.64734006e-01   1.33648860e+00\n",
      "   -2.17895046e-01  -3.45143676e-02   1.19970128e-01  -5.15270054e-01\n",
      "    2.86773294e-02  -1.95191526e+00  -2.24965036e-01  -1.03590041e-02\n",
      "    7.87554383e-01   2.05077723e-01   1.57395080e-01   7.55246639e-01\n",
      "   -1.02746367e+00  -1.05944350e-01   1.64997682e-01  -6.20964170e-02\n",
      "   -2.48403445e-01   2.26829872e-01   1.75832164e+00   1.03758836e+00\n",
      "    7.48565018e-01  -2.26873979e-01  -1.99605480e-01  -9.68821198e-02\n",
      "   -5.02120733e-01  -6.11794591e-02   2.43375674e-01  -7.04360306e-01\n",
      "   -2.11225301e-01  -3.46207321e-02  -2.45310351e-01  -5.93442202e-01\n",
      "   -2.29166597e-02  -2.59968191e-02   1.98546931e-01  -1.55329883e-01\n",
      "    3.76110345e-01  -1.70762300e-01  -1.22522183e-01   1.37593478e-01\n",
      "   -1.84194255e+00  -4.02199149e-01  -1.99203983e-01   9.51337665e-02\n",
      "   -3.67477685e-01   2.28600875e-01  -1.90471411e-02  -1.27159566e-01\n",
      "   -6.28197849e-01   2.31517449e-01  -1.38322741e-01  -1.56607449e-01\n",
      "   -2.24176690e-01   1.27996996e-01  -3.11329067e-02  -1.90741703e-01\n",
      "    1.83533087e-01  -5.60511202e-02  -5.58949709e-01   1.69523692e+00\n",
      "   -7.49916077e-01  -1.61663830e-01  -1.78823084e-01]\n",
      " [  1.55593649e-01   1.57947838e-03  -2.12712455e+00   6.99254036e-01\n",
      "   -4.31568474e-02  -1.93786192e+00   2.08305791e-01  -2.19351083e-01\n",
      "    2.01589897e-01  -6.69227988e-02  -1.79482996e-01   4.94535416e-02\n",
      "   -3.11646312e-02   9.48026925e-02   8.64860296e-01   1.48523390e+00\n",
      "    4.59744036e-03   2.67019719e-02  -1.98830128e-01  -6.55285895e-01\n",
      "    2.08936468e-01  -1.90962219e+00   2.38473997e-01  -7.53404349e-02\n",
      "    9.22233224e-01  -1.59810051e-01  -4.38122451e-02   6.76287353e-01\n",
      "   -9.56009448e-01   7.62051493e-02   2.08316758e-01   2.37420604e-01\n",
      "    4.60751355e-03   1.21164307e-01   1.94472909e+00   1.25153363e+00\n",
      "    1.15108407e+00   2.26254612e-02   1.12005129e-01  -2.02227339e-01\n",
      "   -1.02324712e+00   2.40246207e-02  -5.99274039e-02  -7.38319278e-01\n",
      "    4.54518646e-02  -1.69717625e-01   1.04590669e-01  -5.41544437e-01\n",
      "   -1.41006261e-01  -2.03709334e-01   2.21183762e-01   1.68267637e-02\n",
      "    1.66392219e+00   6.62198514e-02  -1.11394083e+00   4.58147109e-01\n",
      "   -1.84651637e+00  -7.37031698e-01  -3.47018540e-02   1.01663962e-01\n",
      "   -6.76181436e-01  -1.03116497e-01   1.93871096e-01   1.61530659e-01\n",
      "   -4.23650801e-01  -1.78206876e-01   1.69791624e-01  -6.72821999e-02\n",
      "   -2.77484357e-02   8.32941085e-02  -1.47926241e-01  -2.27199569e-01\n",
      "   -2.52190828e-02  -6.67558759e-02  -8.20085108e-01   1.70576537e+00\n",
      "   -5.31638622e-01   1.91028461e-01  -1.04315415e-01]\n",
      " [ -2.06959099e-01   5.44645935e-02  -1.67873466e+00   6.24526858e-01\n",
      "   -2.41457969e-01  -1.94322515e+00   1.07422933e-01   1.51215836e-01\n",
      "   -4.76805270e-02  -7.54523277e-02  -2.20608756e-01   5.73560148e-02\n",
      "   -1.46371782e-01  -4.54481244e-02   5.78561187e-01   1.51381981e+00\n",
      "    5.84721565e-05  -7.82128721e-02  -6.82765245e-03  -6.61005974e-01\n",
      "    1.32795945e-01  -2.05182981e+00   2.37710014e-01  -2.11886317e-02\n",
      "    5.82267106e-01   2.83048898e-02  -8.26483220e-02   7.99181402e-01\n",
      "   -8.45018148e-01   2.45411590e-01   1.65226266e-01  -2.39300773e-01\n",
      "    1.93053916e-01  -1.93865567e-01   2.00083947e+00   1.41021192e+00\n",
      "    6.58667922e-01   5.98940700e-02  -1.06178403e-01   1.23602048e-01\n",
      "   -6.89090490e-01   9.96759981e-02   2.21599594e-01  -9.08219874e-01\n",
      "    2.29391232e-01   4.00688499e-02   1.95389077e-01  -6.54543817e-01\n",
      "   -1.13464475e-01  -1.33928210e-01  -7.47992843e-02   2.62227505e-02\n",
      "   -2.07731780e-02  -2.51866430e-02   2.20185444e-01  -4.37248088e-02\n",
      "   -1.70732188e+00  -2.99239218e-01   1.77523479e-01  -2.37125292e-01\n",
      "   -5.68764448e-01   4.96568233e-02  -6.90223724e-02   9.49945897e-02\n",
      "   -8.39340210e-01  -8.42322558e-02   1.91782966e-01  -7.13214278e-03\n",
      "   -2.24301100e-01  -1.21749982e-01  -1.69244796e-01   1.79846838e-01\n",
      "   -1.62534013e-01  -1.06639251e-01  -6.97434008e-01   1.82384133e+00\n",
      "   -5.48227191e-01   1.09113529e-01  -1.50274202e-01]\n",
      " [  2.01875225e-01   1.03524402e-01  -1.71217728e+00   1.11263394e+00\n",
      "    1.70549437e-01  -2.09575176e+00  -2.44767740e-01  -1.85312867e-01\n",
      "    8.10100883e-02   1.36584267e-01  -1.08750030e-01  -1.25140578e-01\n",
      "   -1.62186533e-01  -1.78440660e-02   7.32740223e-01   1.33661902e+00\n",
      "   -1.17916763e-01  -2.31930256e-01  -9.44626927e-02  -8.37070882e-01\n",
      "    1.73328593e-01  -2.14586806e+00  -1.41485855e-01   1.40300408e-01\n",
      "    7.02834845e-01  -1.96400672e-01   1.08163163e-01   8.61228704e-01\n",
      "   -1.19588721e+00  -1.39401704e-01  -1.23435438e-01   2.43423954e-01\n",
      "    1.36064604e-01  -2.47275442e-01   1.94037080e+00   1.42147470e+00\n",
      "    1.48421466e+00   1.53901428e-02  -1.92210525e-02  -9.92434919e-02\n",
      "   -1.17741108e+00   2.35968783e-01  -1.90633908e-01  -5.34654796e-01\n",
      "    5.46418875e-02  -1.14251450e-01   1.90723553e-01  -6.81573868e-01\n",
      "   -1.47535533e-01   7.33824521e-02  -2.40459546e-01   1.51561007e-01\n",
      "    1.62543070e+00  -1.52348429e-02  -1.19533122e+00   2.32151181e-01\n",
      "   -2.05706787e+00  -7.09038675e-01   6.68096691e-02  -1.40584901e-01\n",
      "   -4.85184371e-01  -1.05564803e-01   1.58159181e-01  -1.92373961e-01\n",
      "   -7.58786500e-01   2.04106286e-01  -2.07344294e-02  -9.46289003e-02\n",
      "   -3.59223634e-02   2.32258156e-01  -8.72742832e-02  -5.94744980e-02\n",
      "   -1.68864399e-01  -2.38659710e-01  -7.37216949e-01   1.67537642e+00\n",
      "   -5.21099865e-01  -2.31825605e-01   2.03336015e-01]\n",
      " [ -5.66006899e-02   8.13580900e-02  -1.67012870e+00   8.99150550e-01\n",
      "   -2.44607568e-01  -2.22285295e+00  -1.57174677e-01  -8.19076449e-02\n",
      "    2.21459135e-01   1.23438343e-01   2.72604376e-02  -7.76246190e-04\n",
      "    3.26453894e-02   1.08698860e-01   7.43356586e-01   1.37343037e+00\n",
      "    1.24419644e-01   2.44268522e-01  -1.15074784e-01  -6.31778121e-01\n",
      "   -2.43183509e-01  -1.83316481e+00   2.02034876e-01   2.00312540e-01\n",
      "    4.48050201e-01   3.57348174e-02  -2.08818913e-03   8.29071224e-01\n",
      "   -1.11509848e+00   1.59624740e-01  -1.61022723e-01  -6.59700781e-02\n",
      "   -1.75160512e-01  -2.35543489e-01   1.86941922e+00   1.39329946e+00\n",
      "    5.88913560e-01   5.59831709e-02  -2.30439663e-01   2.26997182e-01\n",
      "   -5.95404088e-01  -1.00583881e-01  -2.39748105e-01  -5.37131608e-01\n",
      "    1.46283671e-01   7.26767629e-02   7.07104355e-02  -6.90317214e-01\n",
      "    2.23404303e-01  -6.22208714e-02  -2.21127599e-01  -8.92570466e-02\n",
      "    3.59802634e-01   2.36658752e-03  -1.63579345e-01   9.32529569e-02\n",
      "   -1.98050857e+00  -2.82278448e-01  -8.06912333e-02   2.15906307e-01\n",
      "   -6.60395801e-01  -7.28451163e-02   2.32008770e-01  -8.45043063e-02\n",
      "   -6.01171374e-01   1.85928196e-02   4.30283397e-02  -1.61104202e-02\n",
      "    1.66631714e-01  -2.27434158e-01  -1.47276759e-01   1.15192369e-01\n",
      "    2.35894695e-01  -5.23459166e-02  -8.47232163e-01   1.86350060e+00\n",
      "   -4.54373419e-01   1.49412379e-01   2.36387953e-01]\n",
      " [  1.20315596e-01   3.84636372e-02  -1.76784492e+00   8.68436277e-01\n",
      "    2.34456941e-01  -2.07697725e+00  -7.03790784e-02   1.68096870e-02\n",
      "    3.11964899e-02  -3.80806327e-02   1.01745978e-01   6.44607991e-02\n",
      "   -2.42266491e-01  -1.69220477e-01   5.69063306e-01   1.37564373e+00\n",
      "    1.35344490e-01  -2.37064153e-01  -2.12874696e-01  -4.27494675e-01\n",
      "    1.38943329e-01  -2.06360316e+00   1.16962984e-01  -6.42229170e-02\n",
      "    4.77543741e-01   1.63993493e-01   2.40773037e-01   8.13289285e-01\n",
      "   -9.32308912e-01   1.73696712e-01   2.03367606e-01  -3.79874855e-02\n",
      "   -6.37938380e-02   4.61081415e-02   2.13862514e+00   1.08882916e+00\n",
      "    1.58087301e+00  -1.54435754e-01  -2.30366126e-01  -1.93884015e-01\n",
      "   -1.02818346e+00  -2.60513127e-02  -6.15214705e-02  -8.65866899e-01\n",
      "   -1.09641969e-01  -1.29118949e-01  -2.22809255e-01  -7.45932460e-01\n",
      "   -2.10032374e-01   6.16167039e-02  -6.22297078e-02   6.39590472e-02\n",
      "    1.53946030e+00  -1.88619778e-01  -1.34362781e+00   4.65451896e-01\n",
      "   -2.26819801e+00  -7.28638411e-01  -1.26473427e-01   1.94679901e-01\n",
      "   -5.78291655e-01  -2.46782869e-01  -9.84627306e-02   1.90610364e-01\n",
      "   -9.07974660e-01   1.83363259e-03   2.23991796e-01  -1.37822628e-01\n",
      "    1.30571947e-01   9.29173082e-02  -3.45878899e-02   2.40640566e-01\n",
      "   -9.66021717e-02   6.34292215e-02  -5.22035003e-01   1.43659806e+00\n",
      "   -3.53243858e-01  -4.61386889e-02  -7.36425370e-02]\n",
      " [ -7.54075497e-02  -7.08190054e-02  -1.60700071e+00   9.35399175e-01\n",
      "    3.81223112e-02  -2.01399207e+00  -8.37464929e-02   2.04537407e-01\n",
      "    2.37815097e-01   1.66858330e-01  -2.46048003e-01   8.28123242e-02\n",
      "   -4.56862152e-03  -1.47247642e-01   8.61079454e-01   1.70087266e+00\n",
      "    2.33815178e-01  -2.22055882e-01   7.15175122e-02  -4.73622203e-01\n",
      "   -2.03110099e-01  -2.14116263e+00   2.97049433e-02   1.67766944e-01\n",
      "    6.60237968e-01   1.02203682e-01  -2.45941624e-01   7.74165452e-01\n",
      "   -7.16577351e-01  -2.45246440e-01   2.93423384e-02   1.57031998e-01\n",
      "    1.91050023e-02  -2.04099208e-01   1.86620963e+00   1.25236022e+00\n",
      "    5.39206207e-01  -6.81782365e-02  -1.08471513e-03   1.35385409e-01\n",
      "   -9.04112518e-01   2.02733144e-01  -2.13530406e-01  -6.04269683e-01\n",
      "   -2.02626586e-02   1.04871392e-03   1.11646488e-01  -7.52103806e-01\n",
      "    1.70539007e-01   1.95018426e-01  -1.00293756e-02   3.02166492e-02\n",
      "    3.00206989e-01   1.87258378e-01   1.23611748e-01  -4.52419110e-02\n",
      "   -2.13789797e+00  -4.95389044e-01  -1.45963281e-01   1.62701532e-01\n",
      "   -4.11979377e-01  -1.77286237e-01  -9.38540697e-03  -5.21945357e-02\n",
      "   -5.38350582e-01  -3.86332870e-02  -4.41008955e-02   2.85003334e-02\n",
      "    1.58728406e-01   1.15170017e-01  -1.48774713e-01  -9.14260000e-02\n",
      "   -4.80261594e-02   1.80405602e-01  -6.03495598e-01   1.62621760e+00\n",
      "   -6.28385425e-01  -9.84728634e-02   4.27439064e-02]\n",
      " [  8.25275332e-02   4.12519425e-02  -1.77925158e+00   8.65431190e-01\n",
      "    1.45747736e-01  -1.85625064e+00   1.12324908e-01  -2.34747261e-01\n",
      "   -2.44142219e-01   1.11877277e-01   1.39382914e-01  -1.06920913e-01\n",
      "    4.36510295e-02   7.30567425e-02   6.61381781e-01   1.66502988e+00\n",
      "    2.13569537e-01   1.19347706e-01  -1.48819953e-01  -6.49626076e-01\n",
      "   -2.22099766e-01  -1.96442366e+00  -7.95260072e-02   1.07072726e-01\n",
      "    6.84884191e-01  -2.47355446e-01   1.12564281e-01   4.21420425e-01\n",
      "   -8.02041888e-01   3.82825732e-04  -2.88977325e-02   6.19564205e-02\n",
      "    2.03819126e-02   2.39817306e-01   2.05964303e+00   1.33746839e+00\n",
      "    1.26216173e+00   2.23111376e-01  -1.05857491e-01  -1.85824722e-01\n",
      "   -7.68985093e-01   6.48905337e-03  -1.72884062e-01  -8.68668377e-01\n",
      "   -5.12134731e-02  -2.48586968e-01   6.61575347e-02  -4.43941265e-01\n",
      "    1.67096660e-01   1.22805342e-01  -3.86784077e-02   2.21418574e-01\n",
      "    1.70475900e+00  -2.47396350e-01  -1.37686348e+00   3.27329040e-01\n",
      "   -1.85842621e+00  -1.04524541e+00   2.37008408e-01  -1.38658345e-01\n",
      "   -4.75373030e-01  -3.28278542e-02   2.41490707e-01  -2.29607671e-02\n",
      "   -7.70304501e-01   1.25584945e-01   8.15837830e-02   9.02271420e-02\n",
      "    1.29150435e-01  -7.02792853e-02  -8.54499638e-03  -1.30557373e-01\n",
      "    1.37383714e-01  -2.07682043e-01  -8.64049435e-01   1.40628326e+00\n",
      "   -8.19067538e-01   1.93214312e-01  -5.89612275e-02]\n",
      " [  2.29521498e-01   1.63439050e-01  -1.38579595e+00   9.86859918e-01\n",
      "    1.60529092e-01  -2.04851055e+00  -1.46169394e-01   2.39608869e-01\n",
      "   -3.82699668e-02  -1.96963459e-01   1.46639094e-01   2.00556770e-01\n",
      "   -2.17354029e-01   1.81074634e-01   5.07612526e-01   1.68233120e+00\n",
      "    2.34913126e-01  -8.81077647e-02   4.50956076e-02  -7.64984429e-01\n",
      "    1.20716020e-01  -1.86618853e+00  -1.65810913e-01  -1.48405343e-01\n",
      "    9.04256582e-01   9.33383405e-03   2.31501594e-01   8.38779747e-01\n",
      "   -8.38483691e-01   2.35320434e-01   2.35022113e-01  -6.78522140e-02\n",
      "   -1.14277661e-01  -2.27625683e-01   2.12416196e+00   1.30746245e+00\n",
      "    7.63279617e-01   2.33939573e-01  -1.53449953e-01   2.42899403e-01\n",
      "   -7.00419962e-01  -5.03438860e-02   2.22543254e-01  -4.30364966e-01\n",
      "   -6.85539842e-03  -1.37817055e-01   1.17431119e-01  -7.87124038e-01\n",
      "   -2.04277515e-01  -7.70519078e-02  -3.12314928e-03   8.67147595e-02\n",
      "    2.13875115e-01   1.16901472e-01  -1.46722093e-01   1.43796265e-01\n",
      "   -2.09779358e+00  -2.34672606e-01  -1.99671298e-01  -1.94350034e-02\n",
      "   -6.19556189e-01   1.62953585e-02   2.50864476e-02  -1.59751624e-02\n",
      "   -5.94552398e-01   1.54194847e-01   6.91594183e-03  -2.48098075e-01\n",
      "    7.32636899e-02  -2.44737014e-01  -1.17787674e-01  -1.04180813e-01\n",
      "    3.62022966e-02   2.13543639e-01  -8.24200988e-01   1.57709968e+00\n",
      "   -4.24864113e-01  -5.16098738e-02  -1.38023585e-01]]\n",
      " ===== printing paramter: W1_2 ===== \n",
      "[[ 0.08668144  0.13211854 -1.62031138 ..., -0.56268919  0.12954392\n",
      "   0.07331015]\n",
      " [ 0.02961291 -0.20440319 -1.33492386 ..., -0.676099   -0.10150583\n",
      "  -0.15325764]\n",
      " [ 0.06643946 -0.18260102 -1.33882999 ..., -0.47939676 -0.16107579\n",
      "   0.12277727]\n",
      " ..., \n",
      " [-0.04447003  0.13297813 -1.59083927 ...,  0.01426713  0.20046957\n",
      "   0.17449762]\n",
      " [ 0.14888166  0.1121632  -0.02499583 ..., -0.425138    0.18047072\n",
      "   0.10201548]\n",
      " [ 0.14760821 -0.02811867 -1.87792003 ..., -0.58390552 -0.17641452\n",
      "  -0.16554707]]\n",
      " ===== printing paramter: b1 ===== \n",
      "[[ 0.1379858  -0.2426794  -1.54975736  0.70988894 -0.21958113 -2.20390677\n",
      "  -0.02963136  0.22777021 -0.21228601 -0.24139084 -0.18224487 -0.0600369\n",
      "   0.01911399 -0.12768553  0.76834375  1.74459732  0.20635939 -0.11979708\n",
      "  -0.1389616  -0.74662089  0.26373124 -2.11249709 -0.10138898  0.02505478\n",
      "   0.74046928 -0.16530555 -0.14615618  0.62891781 -0.82225335  0.00864062\n",
      "   0.05819967  0.03238523 -0.19809267  0.26666957  1.65419829  1.14132977\n",
      "   0.78517568  0.23243511 -0.05368592 -0.23687318 -0.56212205  0.12555727\n",
      "  -0.19479653 -0.50266361 -0.0542178  -0.10397865  0.15203038 -0.79994655\n",
      "   0.11054882  0.06640151  0.0755015   0.17806098  0.44053265 -0.0106917\n",
      "  -0.13756406 -0.09760361 -1.75031173 -0.43884254 -0.06945609  0.22621322\n",
      "  -0.53373575  0.04204869 -0.17974341 -0.09950233 -0.59404612  0.04244679\n",
      "   0.1361663   0.25355273 -0.04241891 -0.0799779  -0.05772512  0.04730797\n",
      "   0.15825424 -0.11138621 -0.76065743  1.69451427 -0.30100572  0.21520746\n",
      "   0.08508503]]\n",
      " ===== printing paramter: W2_1 ===== \n",
      "[[  1.23116091e-01]\n",
      " [ -1.58904735e-02]\n",
      " [  3.40360373e-01]\n",
      " [ -4.85378288e-04]\n",
      " [ -1.79625854e-01]\n",
      " [ -9.78750512e-02]\n",
      " [ -3.99675556e-02]\n",
      " [  3.10282755e+00]\n",
      " [  2.59325802e-02]\n",
      " [ -1.68904826e-01]\n",
      " [ -2.67238855e-01]\n",
      " [ -3.01506072e-01]\n",
      " [ -2.26593494e-01]\n",
      " [ -1.04000866e-01]\n",
      " [  2.06156790e-01]\n",
      " [  1.54656678e-01]\n",
      " [ -3.05683970e-01]\n",
      " [ -1.17881075e-01]\n",
      " [  1.75227165e-01]\n",
      " [  3.07707012e-01]\n",
      " [ -1.38448313e-01]\n",
      " [ -2.21356168e-01]\n",
      " [  1.71384931e-01]\n",
      " [ -2.01369971e-01]\n",
      " [  2.97802985e-01]\n",
      " [  1.40903443e-01]\n",
      " [  4.90796566e-03]\n",
      " [ -4.31943536e-02]\n",
      " [  2.95774341e-01]\n",
      " [  2.11029232e-01]\n",
      " [  2.58739710e-01]\n",
      " [ -8.49786401e-03]\n",
      " [ -2.59447694e-02]\n",
      " [  2.64570534e-01]\n",
      " [ -9.73253250e-02]\n",
      " [  6.36721551e-02]\n",
      " [ -2.66629398e-01]\n",
      " [ -1.76923931e-01]\n",
      " [ -1.95268631e-01]\n",
      " [ -1.67722285e-01]\n",
      " [ -1.05329230e-01]\n",
      " [  2.36342967e-01]\n",
      " [  2.07212269e-01]\n",
      " [  2.57398307e-01]\n",
      " [  2.89439738e-01]\n",
      " [ -2.70907879e-01]\n",
      " [  1.02707386e-01]\n",
      " [ -1.82634890e-02]\n",
      " [ -1.70183703e-01]\n",
      " [  2.58813500e-01]\n",
      " [ -2.90660828e-01]\n",
      " [  4.75403368e-02]\n",
      " [ -3.08707297e-01]\n",
      " [  1.83357298e-01]\n",
      " [  4.52282429e-01]\n",
      " [ -8.27740803e-02]\n",
      " [ -1.41584337e-01]\n",
      " [ -2.60403633e-01]\n",
      " [ -4.17588621e-01]\n",
      " [ -2.32764423e-01]\n",
      " [  1.88616221e-04]]\n",
      " ===== printing paramter: W2_2 ===== \n",
      "[[ 0.03244624]\n",
      " [ 0.05521946]\n",
      " [-0.28549233]\n",
      " [-0.57840604]\n",
      " [-0.1882048 ]\n",
      " [-1.85860074]\n",
      " [-0.20625734]\n",
      " [-0.02937768]\n",
      " [ 0.22821182]\n",
      " [-0.25465381]\n",
      " [-0.07053591]\n",
      " [ 0.0591194 ]\n",
      " [-0.24907649]\n",
      " [-0.14360291]\n",
      " [ 0.036     ]\n",
      " [ 1.4250493 ]\n",
      " [-0.08628234]\n",
      " [ 0.22030392]\n",
      " [ 0.07685608]\n",
      " [-0.51182073]\n",
      " [ 0.06199834]\n",
      " [ 0.13739091]\n",
      " [-0.1144044 ]\n",
      " [-0.14986318]\n",
      " [ 0.64529806]\n",
      " [ 0.08945617]\n",
      " [ 0.05500334]\n",
      " [-0.05331102]\n",
      " [ 0.80402076]\n",
      " [-0.06348212]\n",
      " [-0.31848297]\n",
      " [-0.09997175]\n",
      " [ 0.09205022]\n",
      " [ 0.2182368 ]\n",
      " [ 0.13148952]\n",
      " [ 4.79579163]\n",
      " [ 0.07277427]\n",
      " [-0.08124048]\n",
      " [-0.06911153]\n",
      " [-0.05352607]\n",
      " [ 0.80791128]\n",
      " [ 0.21368387]\n",
      " [-0.17618144]\n",
      " [-0.43652943]\n",
      " [ 0.2438395 ]\n",
      " [ 0.23833799]\n",
      " [-0.1605819 ]\n",
      " [-0.61904764]\n",
      " [ 0.4167003 ]\n",
      " [-0.09933919]\n",
      " [-0.04473257]\n",
      " [-0.11039887]\n",
      " [-0.17485258]\n",
      " [-0.15357506]\n",
      " [ 0.30045491]\n",
      " [-0.2842072 ]\n",
      " [-0.26963541]\n",
      " [-0.27168623]\n",
      " [ 0.0453197 ]\n",
      " [-0.39186817]\n",
      " [ 0.01633739]\n",
      " [-0.13068224]\n",
      " [-0.31672886]\n",
      " [-0.01767531]\n",
      " [-0.41806525]\n",
      " [-0.23939063]\n",
      " [-0.08718484]\n",
      " [-0.14735484]\n",
      " [-0.18282847]\n",
      " [-0.17615536]\n",
      " [-0.00700808]\n",
      " [-0.19127941]\n",
      " [ 0.85375166]\n",
      " [ 0.37353927]\n",
      " [-0.43136883]\n",
      " [-0.3687993 ]\n",
      " [ 0.07407821]\n",
      " [-0.21643335]\n",
      " [-0.26104009]]\n",
      " ===== printing paramter: b2 ===== \n",
      "[[-0.59789741]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold=1500)\n",
    "import pickle\n",
    "para_names=['W1_1', 'W1_2', 'b1', 'W2_1', 'W2_2', 'b2']\n",
    "for i in range(len(para_names)):\n",
    "    print(' ===== printing paramter: '+ para_names[i] +' ===== ')\n",
    "    print(para_pred[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plug in parameters from tv nn and train optimization nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simulation note:\n",
    "- ignored constant vector for the optimization, i.e. set as zero vector\n",
    "- price_focal trained here has length 61 as in tv_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2539.83,  2539.83,  2539.83,  2539.83,  2539.83,  2539.83,\n",
       "        2539.83,  2539.83,  2539.83,  2539.83,  2539.83,  2539.83,\n",
       "        2539.83,  2539.83,  2539.83,  2539.83,  2539.83,  2539.83,\n",
       "         555.19,   555.19,   555.19,   555.19,   555.19,   555.19,\n",
       "         555.19,   555.19,   555.19,   555.19,   555.19,   555.19,\n",
       "         555.19,   555.19,   555.19,   555.19,   555.19,   555.19,\n",
       "         555.19,   555.19,   555.19,   555.19,   555.19,   555.19,\n",
       "         555.19,   555.19,   555.19,   555.19,   555.19,   555.19,\n",
       "         555.19,   555.19])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2_train[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time \n",
    "#optimize the loss funciton by adding negative sign\n",
    "start_time= time.time()\n",
    "tf.reset_default_graph()\n",
    "#p_f = tf.get_variable('p_f', shape=(1, 61)) \n",
    "\n",
    "p_f = tf.get_variable('p_f', shape=(1, 1)) #corresponding index 3 in \n",
    "\n",
    "W1_1 = tf.constant(paras[0], dtype=tf.float32, name='W1_1' )\n",
    "W1_2 = tf.constant(paras[1], dtype=tf.float32, name='W1_2' )\n",
    "b1 = tf.constant(paras[2], dtype=tf.float32, name='b1' )\n",
    "W2_1 = tf.constant(paras[3], dtype=tf.float32, name='W2_1' )\n",
    "W2_2 = tf.constant(paras[4], dtype=tf.float32, name='W2_2' )\n",
    "b2 = tf.constant(paras[5], dtype=tf.float32, name='b2' )\n",
    "p_sub =  tf.placeholder(shape=(None, 18), dtype=tf.float32)\n",
    "D = tf.matmul(tf.nn.sigmoid(tf.matmul(p_f, W1_2) + tf.matmul(p_sub, W1_1)+b1), W2_2)+tf.matmul(p_f, W2_1)+b2\n",
    "\n",
    "p_loss = tf.reduce_mean(-p_f*D) #self defined loss\n",
    "\n",
    "###parameters\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "gradients = trainer.compute_gradients(p_loss)\n",
    "optimizer = trainer.apply_gradients(gradients)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(493):\n",
    "        #parameters \n",
    "        x1_train = tv_train.iloc[step:step+batch_size, 66:84].values \n",
    "        x1_test = tv_test.iloc[step:step+batch_size, 66:84].values\n",
    "\n",
    "        \n",
    "        loss_tr, _ = sess.run([p_loss, optimizer], feed_dict={p_sub: x1_train})\n",
    "        _test_loss = sess.run([p_loss], feed_dict={p_sub: x1_test})\n",
    "        \n",
    "        # predict\n",
    "        D_pred_tr = sess.run([D], feed_dict={p_sub:x1_train})\n",
    "        D_pred_te = sess.run([D], feed_dict={p_sub:x1_test})\n",
    "        pf_pred_tr = sess.run([p_f], feed_dict={p_sub:x1_train})\n",
    "        pf_pred_te = sess.run([p_f], feed_dict={p_sub:x1_test})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"iter:%d, train_loss: %f, test_loss: %f\"%(step, loss_tr,  _test_loss[0]))\n",
    "            print(\"simulated train_revenue: %f, simulated test_revenue: %f\"%(-loss_tr,  -_test_loss[0]))\n",
    "            print('train D prediction parameters results:', D_pred_tr[0][0])\n",
    "            print('test D prediction parameters results:', D_pred_te[0][0])\n",
    "            print('train p_f prediction parameters results:', pf_pred_tr[0][0])\n",
    "            print('test p_f prediction parameters results:', pf_pred_te[0][0])\n",
    "            \n",
    "end_time=time.time()\n",
    "\n",
    "print('trained total elasped time:', end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0, train_loss: 0.015254, test_loss: -1.931153\n",
      "simulated train_revenue: -0.015254, simulated test_revenue: 1.931153\n",
      "train D prediction parameters results: [ 4.08376312]\n",
      "test D prediction parameters results: [ 4.08376312]\n",
      "train p_f prediction parameters results: [ 0.27519163  0.71603501  0.40172035  0.2465378   0.65532756  0.72448307\n",
      "  0.26340601  0.68892837  0.73887032  0.30339321  0.74439216  0.25574148\n",
      "  0.58710772  0.22948661  0.37563109  0.48797232  0.25176153  0.32835129\n",
      "  0.78147292  0.3213383   0.33693913  0.5167436   0.58838892  0.80850506\n",
      "  0.44930342  0.28909355  0.59339476  0.48936957  0.23207715  0.70006233\n",
      "  0.23459548  0.38393834  0.7051819   0.5100767   0.45062947  0.58456802\n",
      "  0.54376048  0.52921164  0.31723985  0.60836881  0.74959129  0.61383998\n",
      "  0.48322216  0.39377356  0.70933682  0.28857201  0.48687404  0.63943958\n",
      "  0.38513982  0.4271408   0.6301102   0.28774959  0.75927979  0.2149156\n",
      "  0.71745455  0.64149898  0.55777717  0.26488853  0.53295612  0.6465013\n",
      "  0.34597835]\n",
      "test p_f prediction parameters results: [ 0.27519163  0.71603501  0.40172035  0.2465378   0.65532756  0.72448307\n",
      "  0.26340601  0.68892837  0.73887032  0.30339321  0.74439216  0.25574148\n",
      "  0.58710772  0.22948661  0.37563109  0.48797232  0.25176153  0.32835129\n",
      "  0.78147292  0.3213383   0.33693913  0.5167436   0.58838892  0.80850506\n",
      "  0.44930342  0.28909355  0.59339476  0.48936957  0.23207715  0.70006233\n",
      "  0.23459548  0.38393834  0.7051819   0.5100767   0.45062947  0.58456802\n",
      "  0.54376048  0.52921164  0.31723985  0.60836881  0.74959129  0.61383998\n",
      "  0.48322216  0.39377356  0.70933682  0.28857201  0.48687404  0.63943958\n",
      "  0.38513982  0.4271408   0.6301102   0.28774959  0.75927979  0.2149156\n",
      "  0.71745455  0.64149898  0.55777717  0.26488853  0.53295612  0.6465013\n",
      "  0.34597835]\n",
      "iter:100, train_loss: -32877.410156, test_loss: -33660.800781\n",
      "simulated train_revenue: 32877.410156, simulated test_revenue: 33660.800781\n",
      "train D prediction parameters results: [ 659.20294189]\n",
      "test D prediction parameters results: [ 659.63519287]\n",
      "train p_f prediction parameters results: [ 69.28001404  69.73469543  69.41191101  69.27007294  69.67552185\n",
      "  68.87541199  69.28619385  69.67295837  69.76075745  69.32836914\n",
      "  69.75111389  69.03865051 -68.30260468  69.24838257  69.38613892\n",
      " -68.34574127  69.2615509   66.04453278 -68.09355927  69.33564758\n",
      "  69.35146332  68.78456879  65.84838104  69.82216644 -68.25084686\n",
      "  69.30109406  69.61952209 -68.36791229  69.25392914  69.62873077\n",
      "  69.24595642  69.39058685  69.71393585  67.5896759   69.46438599\n",
      "  69.59360504  69.56583405  69.53381348  64.72014618  69.61633301\n",
      "  69.76107025  69.63965607  69.49585724  69.41732025  69.73049927\n",
      "  69.31558228  69.49734497  62.87686157  69.40433502  69.32061005\n",
      " -68.23214722  69.09980011  69.7755127   69.22385406  69.72133636\n",
      "  69.65061188  69.57968903 -68.67276764 -68.40973663  69.66360474\n",
      "  69.36969757]\n",
      "test p_f prediction parameters results: [ 69.28001404  69.73469543  69.41191101  69.27007294  69.67552185\n",
      "  68.87541199  69.28619385  69.67295837  69.76075745  69.32836914\n",
      "  69.75111389  69.03865051 -68.30260468  69.24838257  69.38613892\n",
      " -68.34574127  69.2615509   66.04453278 -68.09355927  69.33564758\n",
      "  69.35146332  68.78456879  65.84838104  69.82216644 -68.25084686\n",
      "  69.30109406  69.61952209 -68.36791229  69.25392914  69.62873077\n",
      "  69.24595642  69.39058685  69.71393585  67.5896759   69.46438599\n",
      "  69.59360504  69.56583405  69.53381348  64.72014618  69.61633301\n",
      "  69.76107025  69.63965607  69.49585724  69.41732025  69.73049927\n",
      "  69.31558228  69.49734497  62.87686157  69.40433502  69.32061005\n",
      " -68.23214722  69.09980011  69.7755127   69.22385406  69.72133636\n",
      "  69.65061188  69.57968903 -68.67276764 -68.40973663  69.66360474\n",
      "  69.36969757]\n",
      "iter:200, train_loss: -159975.890625, test_loss: -161748.203125\n",
      "simulated train_revenue: 159975.890625, simulated test_revenue: 161748.203125\n",
      "train D prediction parameters results: [ 1443.44458008]\n",
      "test D prediction parameters results: [ 1444.49743652]\n",
      "train p_f prediction parameters results: [ 152.14485168  152.58067322  152.26750183  152.09127808  152.51397705\n",
      "  151.51593018  152.10157776  152.5652771   152.59588623  152.14717102\n",
      "  152.61070251  151.76763916 -151.40153503  152.09075928  152.24156189\n",
      " -151.58267212  152.11987305  148.34356689 -151.22523499  152.18882751\n",
      "  152.20075989  151.44154358  148.09901428  152.60592651 -151.7318573\n",
      "  152.09544373  152.43670654 -151.55334473  152.08532715  152.36798096\n",
      "  152.10215759  152.25216675  152.5723877   150.11952209  152.31576538\n",
      "  152.4513855   152.3928833   152.39660645  146.74775696  152.47673035\n",
      "  152.61579895  152.47125244  152.34628296  152.23829651  152.56600952\n",
      "  152.14430237  152.35534668  144.37252808  152.24461365  152.07125854\n",
      " -151.39933777  151.81254578  152.62142944  152.08140564  152.58750916\n",
      "  152.50968933  152.4115448  -151.64305115 -151.36729431  152.46876526\n",
      "  152.18606567]\n",
      "test p_f prediction parameters results: [ 152.14485168  152.58067322  152.26750183  152.09127808  152.51397705\n",
      "  151.51593018  152.10157776  152.5652771   152.59588623  152.14717102\n",
      "  152.61070251  151.76763916 -151.40153503  152.09075928  152.24156189\n",
      " -151.58267212  152.11987305  148.34356689 -151.22523499  152.18882751\n",
      "  152.20075989  151.44154358  148.09901428  152.60592651 -151.7318573\n",
      "  152.09544373  152.43670654 -151.55334473  152.08532715  152.36798096\n",
      "  152.10215759  152.25216675  152.5723877   150.11952209  152.31576538\n",
      "  152.4513855   152.3928833   152.39660645  146.74775696  152.47673035\n",
      "  152.61579895  152.47125244  152.34628296  152.23829651  152.56600952\n",
      "  152.14430237  152.35534668  144.37252808  152.24461365  152.07125854\n",
      " -151.39933777  151.81254578  152.62142944  152.08140564  152.58750916\n",
      "  152.50968933  152.4115448  -151.64305115 -151.36729431  152.46876526\n",
      "  152.18606567]\n",
      "iter:300, train_loss: -384427.062500, test_loss: -387221.312500\n",
      "simulated train_revenue: 384427.062500, simulated test_revenue: 387221.312500\n",
      "train D prediction parameters results: [ 2232.00585938]\n",
      "test D prediction parameters results: [ 2232.37768555]\n",
      "train p_f prediction parameters results: [ 235.51501465  235.94140625  235.62705994  235.42498779  235.85948181\n",
      "  234.65940857  235.445755    235.95098877  235.92758179  235.48995972\n",
      "  235.96792603  235.00457764 -234.95266724  235.43190002  235.60430908\n",
      " -235.18969727  235.48774719  231.34373474 -234.76849365  235.54992676\n",
      "  235.54328918  234.60757446  230.91267395  235.89376831 -235.65081787\n",
      "  235.39021301  235.76940918 -235.19819641  235.41856384  235.63938904\n",
      "  235.46220398  235.61714172  235.93157959  233.19287109  235.66438293\n",
      "  235.80973816  235.71214294  235.75349426  229.64103699  235.83573914\n",
      "  235.97291565  235.82229614  235.69363403  235.55479431  235.89616394\n",
      "  235.49435425  235.71932983  226.82846069  235.58207703  235.33187866\n",
      " -234.93113708  235.08920288  235.96531677  235.4354248   235.95716858\n",
      "  235.87171936  235.73579407 -235.0927124  -234.79588318  235.79547119\n",
      "  235.4984436 ]\n",
      "test p_f prediction parameters results: [ 235.51501465  235.94140625  235.62705994  235.42498779  235.85948181\n",
      "  234.65940857  235.445755    235.95098877  235.92758179  235.48995972\n",
      "  235.96792603  235.00457764 -234.95266724  235.43190002  235.60430908\n",
      " -235.18969727  235.48774719  231.34373474 -234.76849365  235.54992676\n",
      "  235.54328918  234.60757446  230.91267395  235.89376831 -235.65081787\n",
      "  235.39021301  235.76940918 -235.19819641  235.41856384  235.63938904\n",
      "  235.46220398  235.61714172  235.93157959  233.19287109  235.66438293\n",
      "  235.80973816  235.71214294  235.75349426  229.64103699  235.83573914\n",
      "  235.97291565  235.82229614  235.69363403  235.55479431  235.89616394\n",
      "  235.49435425  235.71932983  226.82846069  235.58207703  235.33187866\n",
      " -234.93113708  235.08920288  235.96531677  235.4354248   235.95716858\n",
      "  235.87171936  235.73579407 -235.0927124  -234.79588318  235.79547119\n",
      "  235.4984436 ]\n",
      "iter:400, train_loss: -702253.687500, test_loss: -705878.625000\n",
      "simulated train_revenue: 702253.687500, simulated test_revenue: 705878.625000\n",
      "train D prediction parameters results: [ 3012.96826172]\n",
      "test D prediction parameters results: [ 3013.19726562]\n",
      "train p_f prediction parameters results: [ 318.07928467  318.48495483  318.19143677  317.96038818  318.41647339\n",
      "  317.1362915   317.94943237  318.53979492  318.48617554  318.01037598\n",
      "  318.54168701  317.51049805 -317.65673828  317.98968506  318.16567993\n",
      " -318.05651855  318.04452515  313.49377441 -317.52142334  318.10037231\n",
      "  318.1105957   317.08352661  313.18582153  318.42410278 -318.50909424\n",
      "  317.91738892  318.30752563 -317.95700073  317.97576904  318.12695312\n",
      "  318.02053833  318.18280029  318.49780273  315.58746338  318.22741699\n",
      "  318.37643433  318.27539062  318.33078003  311.53640747  318.40219116\n",
      "  318.53518677  318.35192871  318.26428223  318.11065674  318.46353149\n",
      "  318.01873779  318.27584839  308.31829834  318.14419556  317.83032227\n",
      " -317.77496338  317.53726196  318.52856445  318.00549316  318.52346802\n",
      "  318.43377686  318.29751587 -317.72537231 -317.43667603  318.31292725\n",
      "  318.0524292 ]\n",
      "test p_f prediction parameters results: [ 318.07928467  318.48495483  318.19143677  317.96038818  318.41647339\n",
      "  317.1362915   317.94943237  318.53979492  318.48617554  318.01037598\n",
      "  318.54168701  317.51049805 -317.65673828  317.98968506  318.16567993\n",
      " -318.05651855  318.04452515  313.49377441 -317.52142334  318.10037231\n",
      "  318.1105957   317.08352661  313.18582153  318.42410278 -318.50909424\n",
      "  317.91738892  318.30752563 -317.95700073  317.97576904  318.12695312\n",
      "  318.02053833  318.18280029  318.49780273  315.58746338  318.22741699\n",
      "  318.37643433  318.27539062  318.33078003  311.53640747  318.40219116\n",
      "  318.53518677  318.35192871  318.26428223  318.11065674  318.46353149\n",
      "  318.01873779  318.27584839  308.31829834  318.14419556  317.83032227\n",
      " -317.77496338  317.53726196  318.52856445  318.00549316  318.52346802\n",
      "  318.43377686  318.29751587 -317.72537231 -317.43667603  318.31292725\n",
      "  318.0524292 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained total elasped time: 2.4205403327941895\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "#optimize the loss funciton by adding negative sign\n",
    "start_time= time.time()\n",
    "tf.reset_default_graph()\n",
    "p_f = tf.get_variable('p_f', shape=(1, 61)) \n",
    "\n",
    "\n",
    "W1_1 = tf.constant(paras[0], dtype=tf.float32, name='W1_1' )\n",
    "W1_2 = tf.constant(paras[1], dtype=tf.float32, name='W1_2' )\n",
    "b1 = tf.constant(paras[2], dtype=tf.float32, name='b1' )\n",
    "W2_1 = tf.constant(paras[3], dtype=tf.float32, name='W2_1' )\n",
    "W2_2 = tf.constant(paras[4], dtype=tf.float32, name='W2_2' )\n",
    "b2 = tf.constant(paras[5], dtype=tf.float32, name='b2' )\n",
    "p_sub =  tf.placeholder(shape=(None, 18), dtype=tf.float32)\n",
    "D = tf.matmul(tf.nn.sigmoid(tf.matmul(p_f, W1_2) + tf.matmul(p_sub, W1_1)+b1), W2_2)+tf.matmul(p_f, W2_1)+b2\n",
    "\n",
    "p_loss = tf.reduce_mean(-p_f*D) #self defined loss\n",
    "\n",
    "###parameters\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "gradients = trainer.compute_gradients(p_loss)\n",
    "optimizer = trainer.apply_gradients(gradients)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(493):\n",
    "        #parameters \n",
    "        x1_train = tv_train.iloc[step:step+batch_size, 66:84].values \n",
    "        x1_test = tv_test.iloc[step:step+batch_size, 66:84].values\n",
    "\n",
    "        \n",
    "        loss_tr, _ = sess.run([p_loss, optimizer], feed_dict={p_sub: x1_train})\n",
    "        _test_loss = sess.run([p_loss], feed_dict={p_sub: x1_test})\n",
    "        \n",
    "        # predict\n",
    "        D_pred_tr = sess.run([D], feed_dict={p_sub:x1_train})\n",
    "        D_pred_te = sess.run([D], feed_dict={p_sub:x1_test})\n",
    "        pf_pred_tr = sess.run([p_f], feed_dict={p_sub:x1_train})\n",
    "        pf_pred_te = sess.run([p_f], feed_dict={p_sub:x1_test})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"iter:%d, train_loss: %f, test_loss: %f\"%(step, loss_tr,  _test_loss[0]))\n",
    "            print(\"simulated train_revenue: %f, simulated test_revenue: %f\"%(-loss_tr,  -_test_loss[0]))\n",
    "            print('train D prediction parameters results:', D_pred_tr[0][0])\n",
    "            print('test D prediction parameters results:', D_pred_te[0][0])\n",
    "            print('train p_f prediction parameters results:', pf_pred_tr[0][0])\n",
    "            print('test p_f prediction parameters results:', pf_pred_te[0][0])\n",
    "            \n",
    "end_time=time.time()\n",
    "\n",
    "print('trained total elasped time:', end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
