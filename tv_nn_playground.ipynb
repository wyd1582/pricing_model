{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>17.8441415139561</th>\n",
       "      <th>20.8345047623369</th>\n",
       "      <th>9.75006668486477</th>\n",
       "      <th>21.9059403329483</th>\n",
       "      <th>0.8940934127987</th>\n",
       "      <th>19.2655031401848</th>\n",
       "      <th>24.7296107833937</th>\n",
       "      <th>23.8037970237158</th>\n",
       "      <th>20.2116397220013</th>\n",
       "      <th>...</th>\n",
       "      <th>40.0881646175074</th>\n",
       "      <th>23.5263868530805</th>\n",
       "      <th>28.7622014146842</th>\n",
       "      <th>11.0581663341492</th>\n",
       "      <th>11.5397875284751</th>\n",
       "      <th>31.1640774750065</th>\n",
       "      <th>19.6980902784894</th>\n",
       "      <th>11.8289904620517</th>\n",
       "      <th>21.4887755338747</th>\n",
       "      <th>30.696271484106</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <td>17.844142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.431551</td>\n",
       "      <td>14.944885</td>\n",
       "      <td>12.706567</td>\n",
       "      <td>17.821728</td>\n",
       "      <td>26.259722</td>\n",
       "      <td>30.495361</td>\n",
       "      <td>15.754598</td>\n",
       "      <td>26.961524</td>\n",
       "      <td>...</td>\n",
       "      <td>35.897737</td>\n",
       "      <td>15.332237</td>\n",
       "      <td>22.557722</td>\n",
       "      <td>14.004654</td>\n",
       "      <td>13.610536</td>\n",
       "      <td>25.549684</td>\n",
       "      <td>8.342744</td>\n",
       "      <td>13.359954</td>\n",
       "      <td>11.973057</td>\n",
       "      <td>24.976943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>20.834505</td>\n",
       "      <td>27.431551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.003052</td>\n",
       "      <td>30.231553</td>\n",
       "      <td>20.853681</td>\n",
       "      <td>7.932022</td>\n",
       "      <td>13.322052</td>\n",
       "      <td>31.633801</td>\n",
       "      <td>5.056304</td>\n",
       "      <td>...</td>\n",
       "      <td>45.178950</td>\n",
       "      <td>31.425586</td>\n",
       "      <td>35.515360</td>\n",
       "      <td>23.587277</td>\n",
       "      <td>23.816870</td>\n",
       "      <td>37.487015</td>\n",
       "      <td>28.672135</td>\n",
       "      <td>23.958331</td>\n",
       "      <td>29.930654</td>\n",
       "      <td>37.099025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>9.750067</td>\n",
       "      <td>14.944885</td>\n",
       "      <td>23.003052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.616483</td>\n",
       "      <td>9.708985</td>\n",
       "      <td>21.592207</td>\n",
       "      <td>26.582277</td>\n",
       "      <td>21.715362</td>\n",
       "      <td>22.440459</td>\n",
       "      <td>...</td>\n",
       "      <td>38.884407</td>\n",
       "      <td>21.410910</td>\n",
       "      <td>27.059202</td>\n",
       "      <td>5.217206</td>\n",
       "      <td>6.172754</td>\n",
       "      <td>29.599593</td>\n",
       "      <td>17.115810</td>\n",
       "      <td>6.697852</td>\n",
       "      <td>19.149508</td>\n",
       "      <td>29.106654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>21.905940</td>\n",
       "      <td>12.706567</td>\n",
       "      <td>30.231553</td>\n",
       "      <td>19.616483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.887686</td>\n",
       "      <td>29.172416</td>\n",
       "      <td>33.036705</td>\n",
       "      <td>9.313997</td>\n",
       "      <td>29.805714</td>\n",
       "      <td>...</td>\n",
       "      <td>33.573661</td>\n",
       "      <td>8.580248</td>\n",
       "      <td>18.638509</td>\n",
       "      <td>18.909976</td>\n",
       "      <td>18.619977</td>\n",
       "      <td>22.165954</td>\n",
       "      <td>9.584125</td>\n",
       "      <td>18.437603</td>\n",
       "      <td>4.254732</td>\n",
       "      <td>21.503276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>0.894093</td>\n",
       "      <td>17.821728</td>\n",
       "      <td>20.853681</td>\n",
       "      <td>9.708985</td>\n",
       "      <td>21.887686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.286239</td>\n",
       "      <td>24.745768</td>\n",
       "      <td>23.787000</td>\n",
       "      <td>20.231406</td>\n",
       "      <td>...</td>\n",
       "      <td>40.078193</td>\n",
       "      <td>23.509391</td>\n",
       "      <td>28.748301</td>\n",
       "      <td>11.021962</td>\n",
       "      <td>11.505099</td>\n",
       "      <td>31.151249</td>\n",
       "      <td>19.677788</td>\n",
       "      <td>11.795152</td>\n",
       "      <td>21.470167</td>\n",
       "      <td>30.683248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0.000000   17.844142  20.834505  9.750067   21.905940  0.894093   \\\n",
       "NaN NaN  17.844142   0.000000  27.431551  14.944885  12.706567  17.821728   \n",
       "    NaN  20.834505  27.431551   0.000000  23.003052  30.231553  20.853681   \n",
       "    NaN   9.750067  14.944885  23.003052   0.000000  19.616483   9.708985   \n",
       "    NaN  21.905940  12.706567  30.231553  19.616483   0.000000  21.887686   \n",
       "    NaN   0.894093  17.821728  20.853681   9.708985  21.887686   0.000000   \n",
       "\n",
       "         19.265503  24.729611  23.803797  20.211640    ...      40.088165  \\\n",
       "NaN NaN  26.259722  30.495361  15.754598  26.961524    ...      35.897737   \n",
       "    NaN   7.932022  13.322052  31.633801   5.056304    ...      45.178950   \n",
       "    NaN  21.592207  26.582277  21.715362  22.440459    ...      38.884407   \n",
       "    NaN  29.172416  33.036705   9.313997  29.805714    ...      33.573661   \n",
       "    NaN  19.286239  24.745768  23.787000  20.231406    ...      40.078193   \n",
       "\n",
       "         23.526387  28.762201  11.058166  11.539788  31.164077  19.698090  \\\n",
       "NaN NaN  15.332237  22.557722  14.004654  13.610536  25.549684   8.342744   \n",
       "    NaN  31.425586  35.515360  23.587277  23.816870  37.487015  28.672135   \n",
       "    NaN  21.410910  27.059202   5.217206   6.172754  29.599593  17.115810   \n",
       "    NaN   8.580248  18.638509  18.909976  18.619977  22.165954   9.584125   \n",
       "    NaN  23.509391  28.748301  11.021962  11.505099  31.151249  19.677788   \n",
       "\n",
       "         11.828990  21.488776  30.696271  \n",
       "NaN NaN  13.359954  11.973057  24.976943  \n",
       "    NaN  23.958331  29.930654  37.099025  \n",
       "    NaN   6.697852  19.149508  29.106654  \n",
       "    NaN  18.437603   4.254732  21.503276  \n",
       "    NaN  11.795152  21.470167  30.683248  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#translate from the matlab code\n",
    "import xlrd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "NumSKU=49\n",
    "NumVar=13\n",
    "ID=[1138263,1139362,1139363,1141061,1142731,1143640,1144140,\n",
    "        1148001,1148010,1148081,1162466,1162467,1162557,1162558,\n",
    "        1162559,1163152,1163153,1164313,1164961,1164962,1165757,\n",
    "        1166153,1166984,1166998,1167021,1167087,1167847,1167918,\n",
    "        1170236,1170372,1170739,1173299,1174241,1174242,1174243,\n",
    "        1174244,1174275,1174293,1174299,1174313,1174314,1174315,\n",
    "        1174339,1174340,1175687,1175833,1175835,1175950,1177151]\n",
    "tv_info = pd.read_excel('Television.xlsx', sheet_name='Main')\n",
    "tv_info.head()\n",
    "\n",
    "tv_sim = pd.read_excel('Television.xlsx', sheet_name='Similarity')\n",
    "tv_sim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>SalesQuantity</th>\n",
       "      <th>SalesQuantityLag1</th>\n",
       "      <th>SalesQuantityLag7</th>\n",
       "      <th>SalesQuantityLag14</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "      <th>InventoryAvailability</th>\n",
       "      <th>WeekOfYear</th>\n",
       "      <th>...</th>\n",
       "      <th>Var75</th>\n",
       "      <th>Var76</th>\n",
       "      <th>Var77</th>\n",
       "      <th>Var78</th>\n",
       "      <th>Var79</th>\n",
       "      <th>Var80</th>\n",
       "      <th>Var81</th>\n",
       "      <th>Var82</th>\n",
       "      <th>Var83</th>\n",
       "      <th>Var84</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2626.27000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1779.66000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2329.659900</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2533.055050</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2541.530000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1753.387550</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2651.69398</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1880.51008</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2365.113317</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2555.651667</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2794.132285</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1842.372600</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2774.57500</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1851.69608</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2355.083980</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2725.285017</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2835.449192</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1789.243869</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2795.76000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1876.27505</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2468.837450</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2965.260000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2721.716275</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1905.935100</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2795.76000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1876.27505</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2468.837450</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2626.273333</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3473.720000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1905.925200</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID       Date  SalesQuantity  SalesQuantityLag1  SalesQuantityLag7  \\\n",
       "0   1 2016-01-01              2                  1                  1   \n",
       "1   1 2016-01-02              6                  2                  1   \n",
       "2   1 2016-01-03              5                  6                  1   \n",
       "3   1 2016-01-04              6                  5                  1   \n",
       "4   1 2016-01-05              2                  6                  1   \n",
       "\n",
       "   SalesQuantityLag14       Price  Discount  InventoryAvailability  \\\n",
       "0                   1  2626.27000      1.01                   0.93   \n",
       "1                   1  2651.69398      1.01                   0.93   \n",
       "2                   1  2774.57500      1.01                   0.93   \n",
       "3                   1  2795.76000      1.01                   0.93   \n",
       "4                   1  2795.76000      1.01                   0.93   \n",
       "\n",
       "   WeekOfYear  ...         Var75  Var76        Var77  Var78        Var79  \\\n",
       "0           1  ...    1779.66000   1.01  2329.659900   1.01  2533.055050   \n",
       "1           1  ...    1880.51008   1.01  2365.113317   1.01  2555.651667   \n",
       "2           1  ...    1851.69608   1.01  2355.083980   1.01  2725.285017   \n",
       "3           2  ...    1876.27505   1.01  2468.837450   1.01  2965.260000   \n",
       "4           2  ...    1876.27505   1.01  2468.837450   1.01  2626.273333   \n",
       "\n",
       "   Var80        Var81  Var82        Var83  Var84  \n",
       "0   1.01  2541.530000   1.01  1753.387550   1.01  \n",
       "1   1.01  2794.132285   1.01  1842.372600   1.01  \n",
       "2   1.01  2835.449192   1.01  1789.243869   1.01  \n",
       "3   1.01  2721.716275   1.01  1905.935100   1.01  \n",
       "4   1.01  3473.720000   1.01  1905.925200   1.01  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tv_info = pd.read_excel('C:/Users/wyd15/Downloads/Television.xlsx', sheet_name='new_tv_info') #processed by matlab\n",
    "new_tv_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural network train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import libraries and packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "#translate from the matlab code\n",
    "import xlrd\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import InputLayer, Input\n",
    "from tensorflow.python.keras.layers import Reshape, MaxPooling2D\n",
    "from tensorflow.python.keras.layers import Conv2D, Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "#train data\n",
    "new_tv_info = pd.read_excel('C:/Users/wyd15/Downloads/Television.xlsx', sheet_name='new_tv_info') #processed by matlab\n",
    "tv_sim = pd.read_excel('Television.xlsx', sheet_name='Similarity')\n",
    "#tv_sim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24697\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>SalesQuantity</th>\n",
       "      <th>SalesQuantityLag1</th>\n",
       "      <th>SalesQuantityLag7</th>\n",
       "      <th>SalesQuantityLag14</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "      <th>InventoryAvailability</th>\n",
       "      <th>WeekOfYear</th>\n",
       "      <th>...</th>\n",
       "      <th>Var75</th>\n",
       "      <th>Var76</th>\n",
       "      <th>Var77</th>\n",
       "      <th>Var78</th>\n",
       "      <th>Var79</th>\n",
       "      <th>Var80</th>\n",
       "      <th>Var81</th>\n",
       "      <th>Var82</th>\n",
       "      <th>Var83</th>\n",
       "      <th>Var84</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2626.27000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1779.66000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2329.659900</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2533.055050</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2541.530000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1753.387550</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2651.69398</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1880.51008</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2365.113317</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2555.651667</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2794.132285</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1842.372600</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2774.57500</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1851.69608</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2355.083980</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2725.285017</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2835.449192</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1789.243869</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2795.76000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1876.27505</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2468.837450</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2965.260000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2721.716275</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1905.935100</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2795.76000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1876.27505</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2468.837450</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2626.273333</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3473.720000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1905.925200</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID       Date  SalesQuantity  SalesQuantityLag1  SalesQuantityLag7  \\\n",
       "0   1 2016-01-01              2                  1                  1   \n",
       "1   1 2016-01-02              6                  2                  1   \n",
       "2   1 2016-01-03              5                  6                  1   \n",
       "3   1 2016-01-04              6                  5                  1   \n",
       "4   1 2016-01-05              2                  6                  1   \n",
       "\n",
       "   SalesQuantityLag14       Price  Discount  InventoryAvailability  \\\n",
       "0                   1  2626.27000      1.01                   0.93   \n",
       "1                   1  2651.69398      1.01                   0.93   \n",
       "2                   1  2774.57500      1.01                   0.93   \n",
       "3                   1  2795.76000      1.01                   0.93   \n",
       "4                   1  2795.76000      1.01                   0.93   \n",
       "\n",
       "   WeekOfYear  ...         Var75  Var76        Var77  Var78        Var79  \\\n",
       "0           1  ...    1779.66000   1.01  2329.659900   1.01  2533.055050   \n",
       "1           1  ...    1880.51008   1.01  2365.113317   1.01  2555.651667   \n",
       "2           1  ...    1851.69608   1.01  2355.083980   1.01  2725.285017   \n",
       "3           2  ...    1876.27505   1.01  2468.837450   1.01  2965.260000   \n",
       "4           2  ...    1876.27505   1.01  2468.837450   1.01  2626.273333   \n",
       "\n",
       "   Var80        Var81  Var82        Var83  Var84  \n",
       "0   1.01  2541.530000   1.01  1753.387550   1.01  \n",
       "1   1.01  2794.132285   1.01  1842.372600   1.01  \n",
       "2   1.01  2835.449192   1.01  1789.243869   1.01  \n",
       "3   1.01  2721.716275   1.01  1905.935100   1.01  \n",
       "4   1.01  3473.720000   1.01  1905.925200   1.01  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(new_tv_info))\n",
    "new_tv_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, train_loss: 344525.156250, test_loss: 3664404.750000\n",
      "train parameters results: (18, 79)\n",
      "iter: 100, train_loss: 95.746979, test_loss: 2877.747070\n",
      "train parameters results: (18, 79)\n",
      "iter: 200, train_loss: 21.615267, test_loss: 516.666260\n",
      "train parameters results: (18, 79)\n",
      "iter: 300, train_loss: 1.153113, test_loss: 9.923587\n",
      "train parameters results: (18, 79)\n",
      "iter: 400, train_loss: 0.271600, test_loss: 0.927655\n",
      "train parameters results: (18, 79)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_placeholder(input1_dim=18, input2_dim=61):\n",
    "    return tf.placeholder(shape=(None, input1_dim), dtype=tf.float32), \\\n",
    "           tf.placeholder(shape=(None, input2_dim), dtype=tf.float32), \\\n",
    "           tf.placeholder(shape=(None, ), dtype=tf.float32)\n",
    "\n",
    "\n",
    "def get_output(input1, input2):\n",
    "     \n",
    "    input1_dim = input1.get_shape()[1]\n",
    "    input2_dim = input2.get_shape()[1]\n",
    "    W1_1 = tf.get_variable('W1_1', shape=(input1_dim, input1_dim+input2_dim))\n",
    "    W1_2 = tf.get_variable('W1_2', shape=(input2_dim, input1_dim+input2_dim))\n",
    "    b1 = tf.get_variable('b1', shape=(1, input1_dim+input2_dim))\n",
    "    \n",
    "    o1_1 = tf.matmul(input1, W1_1)\n",
    "    o1_2 = tf.matmul(input2, W1_2)\n",
    "    o1 = o1_1 + o1_2 + b1\n",
    "    o1 = tf.nn.sigmoid(o1)\n",
    "\n",
    "    W2_1 = tf.get_variable('W2_1', shape=(input2_dim, 1))\n",
    "    W2_2 = tf.get_variable('W2_2', shape=(input1_dim+input2_dim, 1))\n",
    "    b2 = tf.get_variable('b2', shape=(1, 1))\n",
    "    o2_1 = tf.matmul(input2, W2_1)\n",
    "    o2_2 = tf.matmul(o1, W2_2)\n",
    "    o2 = o2_1 + o2_2 + b2\n",
    "    o2 = tf.reshape(o2, (-1, ))\n",
    "    \n",
    "    saver = tf.train.Saver([W1_1, W1_2, b1, W2_1, W2_2, b2])\n",
    "    Vars=[W1_1, W1_2, b1, W2_1, W2_2, b2]\n",
    "    \n",
    "    return o2, saver, Vars\n",
    "\n",
    "\n",
    "def get_loss(labels, predictions):\n",
    "    return tf.losses.mean_squared_error(labels, predictions)\n",
    "\n",
    "\n",
    "def main():\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    batch_size = 50\n",
    "    learning_rate = 0.5\n",
    "    tf.reset_default_graph()\n",
    "    x1, x2, y = get_placeholder(18, 61)\n",
    "    output, saver, Vars = get_output(x1, x2)\n",
    "\n",
    "    loss = get_loss(y, output)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    #train-test_splie\n",
    "    msk = np.random.rand(len(new_tv_info)) < 0.85\n",
    "    tv_train = new_tv_info[msk]\n",
    "    tv_test = new_tv_info[~msk]\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for step in range(493):\n",
    "            x1_train = tv_train.iloc[step:step+batch_size, 66:84].values \n",
    "            x2_train = np.concatenate((tv_train.iloc[step:step+batch_size, 3:11].values, tv_train.iloc[step:step+batch_size, 14:67].values), 1) \n",
    "            y_train = tv_train.iloc[step:step+batch_size, 2].values \n",
    "            \n",
    "            x1_test = tv_test.iloc[step:step+batch_size, 66:84].values \n",
    "            x2_test = np.concatenate((tv_test.iloc[step:step+batch_size, 3:11].values, tv_test.iloc[step:step+batch_size, 14:67].values), 1) \n",
    "            y_test = tv_test.iloc[step:step+batch_size, 2].values \n",
    "            \n",
    "            _train_loss, _ = sess.run([loss, train_op],\n",
    "                                   feed_dict={x1: x1_train,\n",
    "                                              x2: x2_train,\n",
    "                                              y: y_train})\n",
    "            \n",
    "            _test_loss= sess.run([loss],\n",
    "                                   feed_dict={x1: x1_test,\n",
    "                                              x2: x2_test,\n",
    "                                              y: y_test})\n",
    "            \n",
    "            #results, _ = sess.run([output]) #print parameters, W1_1 etc..\n",
    "            \n",
    "            if step % 100==0:\n",
    "                saver.save(sess, 'C:/Users/wyd15/Desktop/tv_model/tv_modelslack.ckpt', global_step=step)\n",
    "                print(\"iter: %d, train_loss: %f, test_loss: %f\"%(step, _train_loss,  _test_loss[0]))\n",
    "                print('train parameters results:', Vars[0].eval().shape)\n",
    "                #print('output:', )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0, train_loss: 56379.585938, test_loss: 4947716.000000\n",
      "test prediction parameters results: [array([-2217.45703125, -2225.59375   , -2393.54150391, -2308.11962891,\n",
      "       -2369.24682617, -2341.77075195, -2239.87353516, -2272.66040039,\n",
      "       -2293.28564453, -2243.40673828, -2272.5871582 , -2324.9296875 ,\n",
      "       -2304.40454102, -2347.95166016, -2402.86547852, -2291.9765625 ,\n",
      "       -2284.80322266, -2252.75976562, -2472.20776367, -2198.14331055,\n",
      "       -2282.48754883, -2257.44311523, -2231.95507812, -2234.52172852,\n",
      "       -2222.41796875, -2213.06591797, -2226.78979492, -2248.84301758,\n",
      "       -2175.31762695, -2242.14868164, -2226.65698242, -2190.80761719,\n",
      "       -2160.546875  , -2073.97338867, -2189.89819336, -2153.55688477,\n",
      "       -2113.53540039, -2150.45385742, -1976.93676758, -2134.35839844,\n",
      "       -2280.49755859, -2234.45019531, -2162.3190918 , -2163.09790039,\n",
      "       -2157.06298828, -2087.82324219, -2009.50817871, -1990.70690918,\n",
      "       -1922.87976074, -2084.14697266], dtype=float32)]\n",
      "iter:100, train_loss: 149.887863, test_loss: 397.684967\n",
      "test prediction parameters results: [array([-27.98021698, -27.43686485, -26.06754684, -26.00558853,\n",
      "       -15.19523716, -15.01512051, -15.02382946, -15.01720715,\n",
      "       -15.33522701, -15.43175411, -15.48606777, -15.67276287,\n",
      "       -16.13858795, -16.51840401, -16.40924644, -16.47789764,\n",
      "       -16.71856117, -17.28226471, -17.38947678,  28.69902611,\n",
      "        26.33597374,  30.83111763,  31.320467  ,  26.5957737 ,\n",
      "        28.26277733,  28.93488884,  26.13222313,  26.4023571 ,\n",
      "        19.13950539,  28.71094322,  19.11610985,  23.53746605,\n",
      "        15.76229095,  11.06825256,  16.87646294,  11.79800034,\n",
      "        16.83753204,   9.47390366,  20.55798912,   7.65328455,\n",
      "        14.70867157,  13.15289116,  12.57922459,  17.30490494,\n",
      "        21.13203239,  19.24073792,  19.19703102,  17.94743919,\n",
      "        18.05659676,  17.89312553], dtype=float32)]\n",
      "iter:200, train_loss: 3.600749, test_loss: 105.054337\n",
      "test prediction parameters results: [array([ -5.36308432,  -5.76802588,  -6.33744955,  -7.18426847,\n",
      "        -7.56706762,  -7.73917055,  -8.02241898,  -2.85950994,\n",
      "        -0.76639795,  -0.65630007, -19.16545296, -19.06882858,\n",
      "       -19.44869995, -19.12022781, -19.62021828, -19.39723587,\n",
      "         1.80728579,   1.52403688,   1.33068514,   0.95754099,\n",
      "         1.0686841 ,   0.67429209,   0.5021863 ,  -0.99966383,\n",
      "        -2.26816511,  -2.15806532,  -1.28437567,  -0.30970526,\n",
      "        12.74430656,  12.04432297,  11.63938141,   8.86956978,\n",
      "         9.7365284 ,   8.46462822,   8.41030312,   7.90972376,\n",
      "         8.01982307,   7.51532984,   7.62647486,   7.84380579,\n",
      "         7.45332575,  15.18048096,  15.70291138,  16.67946625,\n",
      "        20.47032166,  19.3759861 ,  17.68588829,  15.32151222,\n",
      "        16.91354942,  13.86927032], dtype=float32)]\n",
      "iter:300, train_loss: 1.877277, test_loss: 26.191425\n",
      "test prediction parameters results: [array([ 3.41108441,  3.55471539,  3.00594783,  2.87476802,  3.23627114,\n",
      "        2.9301002 ,  2.55046248,  4.23360348,  3.08457494,  3.42328668,\n",
      "        3.41961598,  3.00872731,  3.06213689,  3.34743905,  2.9708035 ,\n",
      "        4.05566978,  3.8664825 ,  3.87411475,  3.97982192,  4.11019039,\n",
      "        3.79063487, -5.96219158, -6.11500263, -6.38976669, -5.92498398,\n",
      "       -5.97054005, -4.90279198, -4.759161  , -4.62512207, -6.02182674,\n",
      "       -6.06342125, -7.38974667, -6.23841763, -6.5248003 , -6.62618923,\n",
      "       -6.74759293, -6.51820278, -0.88775396, -0.58362269, -0.71470714,\n",
      "       -1.02663064, -5.34246635, -5.92536926, -5.99155235, -5.65712738,\n",
      "       -6.10593987, -5.80515194, -5.80874348, -1.68927026,  1.11075127], dtype=float32)]\n",
      "iter:400, train_loss: 0.278943, test_loss: 0.605521\n",
      "test prediction parameters results: [array([ 0.48584092,  2.55194616,  2.65969276,  2.64618397,  0.19978464,\n",
      "        0.65169007,  0.26905489,  0.35042524,  0.32593775,  0.16953599,\n",
      "        0.18364775,  0.26167738,  0.38454103,  0.31212437, -0.04038131,\n",
      "        0.17761743, -0.01763093,  0.12762487,  2.42963314,  2.1552701 ,\n",
      "        2.13059664,  2.21379089,  1.43860209,  1.28974378,  1.38522851,\n",
      "        1.91331565,  1.42547143,  1.39210689,  1.13534474,  1.27439737,\n",
      "        1.0560019 ,  1.17748249,  1.04427052,  1.00260198,  1.28974378,\n",
      "        1.11041987,  1.00813651,  1.07507479,  0.86900777,  0.92651248,\n",
      "        1.20127559,  0.92533773,  0.79191661,  0.87823188,  0.96005917,\n",
      "        0.8319447 ,  0.62361711,  0.53328979,  0.5077467 ,  0.57253188], dtype=float32)]\n",
      "last test prediction parameters results: [array([ 3.29735518,  1.85655069,  2.56959724,  0.95375288,  2.50964737,\n",
      "        1.46468782,  0.57055849,  0.48522294, -0.12474322,  1.86742628,\n",
      "        1.82249546,  0.0726589 , -0.1878829 , -0.41906321, -0.6261996 ,\n",
      "       -0.45418859, -0.46424341, -0.23836803, -0.16835845, -0.37492442,\n",
      "       -0.2501452 ,  1.55839539,  1.97434139,  1.45882988,  1.41222858,\n",
      "       -0.2403419 , -0.07595015, -0.21189773, -0.55594742, -0.07234883,\n",
      "       -0.64247048, -0.68296731, -0.33832622, -0.46582687, -0.30029988,\n",
      "       -0.56138957, -0.17643416, -0.20577514, -0.38271427, -0.53657305,\n",
      "       -0.55916846, -0.72986305,  1.49461997,  1.44657195,  1.20536709,\n",
      "        0.20210934,  0.32623458,  1.07994366,  0.08936393, -0.02812421], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# tensorflow nn\n",
    "#unfolded version\n",
    "#reference code: https://github.com/yl3829/Spring2018-Project5-grp5/blob/master/lib/model_tb.py  \n",
    "\n",
    "# D = tf.placeholder(dytpe=np.floate32)\n",
    "# w1_1 = tf.Variable((18,79),name='W1_1')\n",
    "# w1_2 = tf.Variable((61,79),name='W1_2')\n",
    "# b1 = tf.Variable((79,),name='b1')\n",
    "# param = [w1_1,w1_2,b1]\n",
    "tf.reset_default_graph()\n",
    "x1=tf.placeholder(shape=(None, 18), dtype=tf.float32)\n",
    "x2=tf.placeholder(shape=(None, 61), dtype=tf.float32)\n",
    "y =tf.placeholder(shape=(None, ), dtype=tf.float32)\n",
    "\n",
    "#\n",
    "W1_1 = tf.get_variable('W1_1', shape=(18, 79))\n",
    "W1_2 = tf.get_variable('W1_2', shape=(61, 79))\n",
    "b1 = tf.get_variable('b1', shape=(1, 79))\n",
    "    \n",
    "o1_1 = tf.matmul(x1, W1_1)\n",
    "o1_2 = tf.matmul(x2, W1_2)\n",
    "o1 = o1_1 + o1_2 + b1\n",
    "o1 = tf.nn.sigmoid(o1)\n",
    "\n",
    "W2_1 = tf.get_variable('W2_1', shape=(61, 1))\n",
    "W2_2 = tf.get_variable('W2_2', shape=(79, 1))\n",
    "b2 = tf.get_variable('b2', shape=(1, 1))\n",
    "\n",
    "o2_1 = tf.matmul(x2, W2_1)\n",
    "o2_2 = tf.matmul(o1, W2_2)\n",
    "o2 = o2_1 + o2_2 + b2\n",
    "o2 = tf.reshape(o2, (-1, ))\n",
    "\n",
    "saver = tf.train.Saver([W1_1, W1_2, b1, W2_1, W2_2, b2])\n",
    "#\n",
    "\n",
    "loss = tf.losses.mean_squared_error(y, o2)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.5)\n",
    "train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "# layer1_out  = tf.nn.sigmoid(tf.matmul(input1,w1_1) + tf.matmul(input2,w1_2) + b1) # tanh would be better\n",
    "# # example to minimize layer1 out put\n",
    "# losses = layer2_out - D\n",
    "# loss = tf.reduce_mean(losses)\n",
    "# trainer = tf.train.AdamOptimizer()\n",
    "# gradients = trainer.compute_gradients(self.loss)\n",
    "# optimizer = trainer.apply_gradients(gradients)\n",
    "\n",
    "#train-test_splie\n",
    "msk = np.random.rand(len(new_tv_info)) < 0.85\n",
    "tv_train = new_tv_info[msk]\n",
    "tv_test = new_tv_info[~msk]\n",
    "batch_size=50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(493):\n",
    "        x1_train = tv_train.iloc[step:step+batch_size, 66:84].values \n",
    "        x2_train = np.concatenate((tv_train.iloc[step:step+batch_size, 3:11].values, tv_train.iloc[step:step+batch_size, 14:67].values), 1) \n",
    "        y_train = tv_train.iloc[step:step+batch_size, 2].values \n",
    "            \n",
    "        x1_test = tv_test.iloc[step:step+batch_size, 66:84].values \n",
    "        x2_test = np.concatenate((tv_test.iloc[step:step+batch_size, 3:11].values, tv_test.iloc[step:step+batch_size, 14:67].values), 1) \n",
    "        y_test = tv_test.iloc[step:step+batch_size, 2].values \n",
    "            \n",
    "        _train_loss, _ = sess.run([loss, train_op],feed_dict={x1: x1_train,\n",
    "                                              x2: x2_train,\n",
    "                                              y: y_train})\n",
    "            \n",
    "        _test_loss= sess.run([loss],feed_dict={x1: x1_test,\n",
    "                                              x2: x2_test,\n",
    "                                              y: y_test})\n",
    "        # predict\n",
    "        D_predict = sess.run([o2], feed_dict={x1:x1_test,x2:x2_test})\n",
    "        para_pred = sess.run([W1_1, W1_2, b1, W2_1, W2_2, b2], feed_dict={x1:x1_test,x2:x2_test})\n",
    "        #results, _ = sess.run([output]) #print parameters, W1_1 etc..\n",
    "            \n",
    "        if step % 100==0:\n",
    "            saver.save(sess, 'C:/Users/wyd15/Desktop/tv_model/tv_modelslack.ckpt', global_step=step)\n",
    "            print(\"iter:%d, train_loss: %f, test_loss: %f\"%(step, _train_loss,  _test_loss[0]))\n",
    "            print('test prediction parameters results:', D_predict)\n",
    "            #print('predicted parameters:', para_pred)\n",
    "    print('last test prediction parameters results:', D_predict)\n",
    "    paras=para_pred\n",
    "                #print('output:', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul:0\", shape=(?, 79), dtype=float32) Tensor(\"MatMul_3:0\", shape=(?, 1), dtype=float32) Tensor(\"Sigmoid:0\", shape=(?, 79), dtype=float32) Tensor(\"Reshape:0\", shape=(?,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(o1_1, o2_2, o1, o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.12797862, -0.02464183, -0.15865442, ...,  0.07635032,\n",
       "         -0.03283224,  8.78123856],\n",
       "        [-0.14371137,  0.23066618,  0.16623376, ...,  0.23077138,\n",
       "          0.01492186,  9.13104439],\n",
       "        [-0.20466076,  0.02212043,  0.24548675, ...,  0.08024786,\n",
       "          0.14321209,  8.84194374],\n",
       "        ..., \n",
       "        [-0.18475252,  0.1449744 ,  0.18428163, ...,  0.18591003,\n",
       "         -0.09214811,  9.03209114],\n",
       "        [ 0.06817509, -0.15058415, -0.1208737 , ..., -0.03315042,\n",
       "         -0.05451523,  9.15025711],\n",
       "        [-0.17511621,  0.19966353,  0.14326422, ...,  0.23431529,\n",
       "          0.03261064,  8.78793621]], dtype=float32),\n",
       " array([[-0.03985462, -0.19516602, -0.01409601, ...,  0.15827347,\n",
       "         -0.02134897,  9.12770748],\n",
       "        [-0.09720873, -0.03819266, -0.20685363, ..., -0.19968246,\n",
       "         -0.01345135,  9.13354969],\n",
       "        [ 0.08976175, -0.13287601,  0.15176423, ..., -0.19269237,\n",
       "          0.19692053,  8.79755592],\n",
       "        ..., \n",
       "        [ 0.04263309, -0.11816388, -0.13857107, ..., -0.07405436,\n",
       "         -0.03310677, -0.15585843],\n",
       "        [-0.10720734, -0.15139839,  0.14744388, ...,  0.14965741,\n",
       "         -0.16488445,  0.12070371],\n",
       "        [-0.11335835,  0.18344562,  0.17625396, ..., -0.03939515,\n",
       "          0.04789694,  9.01753712]], dtype=float32),\n",
       " array([[ -1.28880143e-01,   9.13262367e-02,  -1.52415425e-01,\n",
       "          -7.80011564e-02,   2.66598225e-01,  -2.01258704e-01,\n",
       "           2.57942438e+00,   1.25757068e-01,  -2.28653878e-01,\n",
       "          -3.27113080e+00,  -3.21315527e+00,  -6.64042011e-02,\n",
       "           1.60042703e-01,  -3.60129571e+00,  -1.06340185e-01,\n",
       "          -6.69215471e-02,  -3.92175078e-01,  -1.24579251e-01,\n",
       "           4.66892421e-02,  -3.22398990e-02,   9.91152763e-01,\n",
       "          -4.08292484e+00,   2.44522989e-01,   2.09165037e-01,\n",
       "           2.09186733e-01,  -2.51727849e-02,  -1.83758438e-01,\n",
       "           2.48069227e-01,  -2.36231014e-01,  -2.45451808e-01,\n",
       "          -1.76694661e-01,  -2.65195400e-01,   6.24118745e-02,\n",
       "           5.92141449e-02,   8.81648731e+00,  -2.94944572e+00,\n",
       "          -8.88732076e-03,  -1.75190553e-01,  -2.20410034e-01,\n",
       "           2.43190765e-01,  -7.29043037e-02,  -8.70623887e-02,\n",
       "           1.63508326e-01,  -9.36285108e-02,  -1.04640521e-01,\n",
       "          -2.42362082e-01,  -2.93166339e-02,  -1.88734084e-01,\n",
       "           3.36424708e+00,  -2.24037290e-01,   1.36534065e-01,\n",
       "          -3.59085500e-02,  -5.19813585e+00,  -8.54306966e-02,\n",
       "          -2.50434130e-01,   2.65762687e-01,   1.45312786e-01,\n",
       "          -4.73778456e-01,   2.15791106e-01,   1.29957676e-01,\n",
       "          -1.39987707e-01,  -2.44318992e-02,   5.94207942e-02,\n",
       "          -9.22319293e-03,   2.92110586e+00,   2.31056452e-01,\n",
       "           1.17747247e-01,  -2.96663754e-02,  -4.03133929e-02,\n",
       "           1.08844817e-01,  -1.51900575e-01,  -5.99324703e-05,\n",
       "          -2.32602179e-01,  -1.41889632e-01,  -1.44107386e-01,\n",
       "          -6.18523471e-02,  -1.19278431e-02,  -2.04455942e-01,\n",
       "           8.89628887e+00]], dtype=float32),\n",
       " array([[  2.15664372e-01],\n",
       "        [  2.95043867e-02],\n",
       "        [  3.37305814e-01],\n",
       "        [ -3.54482065e-04],\n",
       "        [  2.24713031e-02],\n",
       "        [ -2.36540541e-01],\n",
       "        [ -2.03332715e-02],\n",
       "        [  1.95428085e+00],\n",
       "        [  2.09402144e-01],\n",
       "        [ -2.80030102e-01],\n",
       "        [ -1.01419732e-01],\n",
       "        [ -2.38751888e-01],\n",
       "        [ -3.32590342e-02],\n",
       "        [ -7.45254606e-02],\n",
       "        [ -1.52041331e-01],\n",
       "        [  9.92755294e-02],\n",
       "        [  1.71738207e-01],\n",
       "        [ -9.57493186e-02],\n",
       "        [ -1.20479494e-01],\n",
       "        [  1.55475944e-01],\n",
       "        [ -9.82978940e-03],\n",
       "        [ -2.17073679e-01],\n",
       "        [  1.47416711e-01],\n",
       "        [ -1.02129519e-01],\n",
       "        [ -2.43092090e-01],\n",
       "        [  2.31292009e-01],\n",
       "        [  2.36384809e-01],\n",
       "        [  2.37142563e-01],\n",
       "        [  1.76510811e-02],\n",
       "        [  2.62824237e-01],\n",
       "        [ -1.17800757e-01],\n",
       "        [ -4.79929149e-02],\n",
       "        [  8.76299441e-02],\n",
       "        [ -1.54720306e-01],\n",
       "        [  1.91961527e-02],\n",
       "        [  3.09042871e-01],\n",
       "        [  5.56714535e-02],\n",
       "        [ -2.68763095e-01],\n",
       "        [  3.08486581e-01],\n",
       "        [ -1.61145821e-01],\n",
       "        [  3.06005418e-01],\n",
       "        [ -2.55234152e-01],\n",
       "        [ -2.05627084e-01],\n",
       "        [ -1.56848729e-02],\n",
       "        [  5.18844724e-02],\n",
       "        [  2.00290203e-01],\n",
       "        [ -3.04167509e-01],\n",
       "        [ -2.11405635e-01],\n",
       "        [ -1.35725588e-01],\n",
       "        [  4.82956767e-02],\n",
       "        [  2.68960357e-01],\n",
       "        [  2.49415636e-01],\n",
       "        [  1.82641953e-01],\n",
       "        [  1.74896389e-01],\n",
       "        [  9.44031700e-02],\n",
       "        [  9.70971510e-02],\n",
       "        [ -1.62215397e-01],\n",
       "        [ -1.23038962e-01],\n",
       "        [ -7.90686384e-02],\n",
       "        [ -1.26589641e-01],\n",
       "        [ -8.28896591e-05]], dtype=float32),\n",
       " array([[ -7.75031298e-02],\n",
       "        [ -7.23425299e-02],\n",
       "        [ -2.44786039e-01],\n",
       "        [ -1.82501078e-01],\n",
       "        [  2.06926003e-01],\n",
       "        [ -1.69141307e-01],\n",
       "        [ -2.00671330e-01],\n",
       "        [ -2.33596936e-01],\n",
       "        [ -1.68653861e-01],\n",
       "        [  4.47464794e-01],\n",
       "        [  1.55362308e-01],\n",
       "        [  5.40989971e+00],\n",
       "        [  3.86104397e-02],\n",
       "        [  6.63052425e-02],\n",
       "        [ -2.11220697e-01],\n",
       "        [ -5.85068651e-02],\n",
       "        [ -7.35945606e+00],\n",
       "        [ -1.29485086e-01],\n",
       "        [  2.38705158e-01],\n",
       "        [  1.09447718e-01],\n",
       "        [ -1.61566898e-01],\n",
       "        [  1.91234231e-01],\n",
       "        [  1.28651997e-02],\n",
       "        [ -2.29963541e-01],\n",
       "        [ -2.11724237e-01],\n",
       "        [ -1.18879050e-01],\n",
       "        [  1.92309171e-01],\n",
       "        [  1.27506286e-01],\n",
       "        [  2.38267243e-01],\n",
       "        [ -1.24703988e-01],\n",
       "        [ -5.30044436e-02],\n",
       "        [ -2.23504201e-01],\n",
       "        [  3.07703018e-03],\n",
       "        [ -1.81806073e-01],\n",
       "        [  2.34306502e+00],\n",
       "        [  2.97046638e+00],\n",
       "        [ -1.36247024e-01],\n",
       "        [ -5.46039315e-03],\n",
       "        [ -2.01925874e-01],\n",
       "        [  2.24427179e-01],\n",
       "        [  2.32229337e-01],\n",
       "        [ -5.27931601e-02],\n",
       "        [ -1.68890193e-01],\n",
       "        [  2.32868537e-01],\n",
       "        [  1.41057000e-01],\n",
       "        [  7.76764750e-02],\n",
       "        [ -2.24799171e-01],\n",
       "        [ -2.91790187e-01],\n",
       "        [ -3.32765269e+00],\n",
       "        [  2.19251603e-01],\n",
       "        [  1.61561280e-01],\n",
       "        [ -2.52479643e-01],\n",
       "        [ -1.47889897e-01],\n",
       "        [ -1.34020805e+00],\n",
       "        [  2.16167457e-02],\n",
       "        [  1.69929817e-01],\n",
       "        [ -1.07934013e-01],\n",
       "        [ -2.30635367e-02],\n",
       "        [  2.18564838e-01],\n",
       "        [  2.01932335e+00],\n",
       "        [  2.05530739e+00],\n",
       "        [ -2.43220404e-01],\n",
       "        [  9.40452982e-03],\n",
       "        [  2.01470315e-01],\n",
       "        [ -1.92971230e-01],\n",
       "        [ -9.87328440e-02],\n",
       "        [ -2.69992858e-01],\n",
       "        [ -3.21544576e+00],\n",
       "        [ -2.50534683e-01],\n",
       "        [  1.42958546e+00],\n",
       "        [  8.08564723e-02],\n",
       "        [  4.68843058e-02],\n",
       "        [ -1.19259968e-01],\n",
       "        [  3.82808298e-02],\n",
       "        [ -2.72085607e-01],\n",
       "        [  3.96944928e+00],\n",
       "        [  4.73595262e-02],\n",
       "        [  1.25783443e-01],\n",
       "        [  4.20193374e-01]], dtype=float32),\n",
       " array([[ 1.5219878]], dtype=float32)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0, train_loss: 0.010566, test_loss: 0.608954\n",
      "train D prediction parameters results: [-1.55027974]\n",
      "test D prediction parameters results: [-1.51575053]\n",
      "train p_f prediction parameters results: [ 0.67228687  0.2296139  -0.29587755  0.28358087  0.19866329  0.62586623\n",
      "  0.58192712 -0.78725988  0.23486775  0.48170024  0.42531598  0.70966125\n",
      "  0.49903339  0.63599986  0.42379194  0.69886136  0.5534876   0.63533586\n",
      "  0.54864156  0.24417377  0.61396378  0.34163204  0.45218825  0.72521758\n",
      "  0.36856967  0.33998907  0.66300887  0.7792201   0.29396209  0.42534399\n",
      "  0.38760504  0.63766384  0.80229014  0.50336373  0.28535783 -0.80295902\n",
      "  0.41856354  0.21278191 -0.49990118  0.49705222 -0.73471856  0.36634728\n",
      "  0.74740744  0.26325041  0.3714588   0.44964105  0.52036899  0.68820477\n",
      "  0.49722019  0.24486715  0.22733545  0.68728238  0.51375556  0.33253378\n",
      "  0.30912346  0.30070296  0.74994135  0.54959834  0.61693633  0.7617718\n",
      "  0.46830419]\n",
      "test p_f prediction parameters results: [ 0.67228687  0.2296139  -0.29587755  0.28358087  0.19866329  0.62586623\n",
      "  0.58192712 -0.78725988  0.23486775  0.48170024  0.42531598  0.70966125\n",
      "  0.49903339  0.63599986  0.42379194  0.69886136  0.5534876   0.63533586\n",
      "  0.54864156  0.24417377  0.61396378  0.34163204  0.45218825  0.72521758\n",
      "  0.36856967  0.33998907  0.66300887  0.7792201   0.29396209  0.42534399\n",
      "  0.38760504  0.63766384  0.80229014  0.50336373  0.28535783 -0.80295902\n",
      "  0.41856354  0.21278191 -0.49990118  0.49705222 -0.73471856  0.36634728\n",
      "  0.74740744  0.26325041  0.3714588   0.44964105  0.52036899  0.68820477\n",
      "  0.49722019  0.24486715  0.22733545  0.68728238  0.51375556  0.33253378\n",
      "  0.30912346  0.30070296  0.74994135  0.54959834  0.61693633  0.7617718\n",
      "  0.46830419]\n",
      "iter:100, train_loss: -23143.515625, test_loss: -23700.097656\n",
      "train D prediction parameters results: [ 511.1109314]\n",
      "test D prediction parameters results: [ 511.1109314]\n",
      "train p_f prediction parameters results: [ 69.89045715  68.5027771   68.94913483  68.28155518  68.40526581\n",
      " -66.19631958  68.38420868  68.48413849  69.45089722 -67.69618988\n",
      "  67.26055145 -66.22826385  68.16687012  67.83929443  65.77030945\n",
      "  69.87343597  69.75476837  67.55644226  67.03917694  69.4378891\n",
      "  68.52111816 -64.98455811  69.6421814   67.54961395 -66.77554321\n",
      "  69.56301117  69.88757324  70.00392914  68.45652771  69.65676117\n",
      "  66.93330383  68.1467514   69.97937012  65.65198517  68.46195984\n",
      "  68.43764496  69.10014343 -67.74594116  68.74061584  64.96650696\n",
      "  68.50532532 -67.23373413 -62.97242355  68.11231232  68.95817566\n",
      "  69.66236115 -67.97589111 -63.94881058  66.59243774  68.7585907\n",
      "  69.46018219  69.91526794  69.71968842  69.53516388  69.48444366\n",
      "  69.47550964  65.06227112  66.98339081  67.76358032  67.1129837\n",
      "  68.46887207]\n",
      "test p_f prediction parameters results: [ 69.89045715  68.5027771   68.94913483  68.28155518  68.40526581\n",
      " -66.19631958  68.38420868  68.48413849  69.45089722 -67.69618988\n",
      "  67.26055145 -66.22826385  68.16687012  67.83929443  65.77030945\n",
      "  69.87343597  69.75476837  67.55644226  67.03917694  69.4378891\n",
      "  68.52111816 -64.98455811  69.6421814   67.54961395 -66.77554321\n",
      "  69.56301117  69.88757324  70.00392914  68.45652771  69.65676117\n",
      "  66.93330383  68.1467514   69.97937012  65.65198517  68.46195984\n",
      "  68.43764496  69.10014343 -67.74594116  68.74061584  64.96650696\n",
      "  68.50532532 -67.23373413 -62.97242355  68.11231232  68.95817566\n",
      "  69.66236115 -67.97589111 -63.94881058  66.59243774  68.7585907\n",
      "  69.46018219  69.91526794  69.71968842  69.53516388  69.48444366\n",
      "  69.47550964  65.06227112  66.98339081  67.76358032  67.1129837\n",
      "  68.46887207]\n",
      "iter:200, train_loss: -114036.500000, test_loss: -115300.945312\n",
      "train D prediction parameters results: [ 1127.34191895]\n",
      "test D prediction parameters results: [ 1127.34191895]\n",
      "train p_f prediction parameters results: [ 153.07151794  151.68377686  152.13023376  151.4624176   151.58625793\n",
      " -149.37677002  151.56509399  151.66526794  152.63194275 -150.87695312\n",
      "  150.4405365  -149.40811157  151.34747314  151.01974487  148.94381714\n",
      "  153.05444336  152.93586731  150.73669434  150.21853638  152.61885071\n",
      "  151.70205688 -148.16261292  152.82321167  150.72984314 -149.95562744\n",
      "  152.74403381  153.06869507  153.18495178  151.63731384  152.83789062\n",
      "  150.11236572  151.32730103  153.16036987  148.82489014  151.64280701\n",
      "  151.6187439   152.28111267 -150.92649841  151.92170715  148.13340759\n",
      "  151.68637085 -150.41459656 -146.14602661  151.29307556  152.1390686\n",
      "  152.84335327 -151.15698242 -147.12521362  149.77001953  151.9394989\n",
      "  152.64131165  153.09629822  152.90077209  152.71621704  152.66537476\n",
      "  152.65647888  148.22608948  150.16203308  150.94381714  150.29222107\n",
      "  151.64981079]\n",
      "test p_f prediction parameters results: [ 153.07151794  151.68377686  152.13023376  151.4624176   151.58625793\n",
      " -149.37677002  151.56509399  151.66526794  152.63194275 -150.87695312\n",
      "  150.4405365  -149.40811157  151.34747314  151.01974487  148.94381714\n",
      "  153.05444336  152.93586731  150.73669434  150.21853638  152.61885071\n",
      "  151.70205688 -148.16261292  152.82321167  150.72984314 -149.95562744\n",
      "  152.74403381  153.06869507  153.18495178  151.63731384  152.83789062\n",
      "  150.11236572  151.32730103  153.16036987  148.82489014  151.64280701\n",
      "  151.6187439   152.28111267 -150.92649841  151.92170715  148.13340759\n",
      "  151.68637085 -150.41459656 -146.14602661  151.29307556  152.1390686\n",
      "  152.84335327 -151.15698242 -147.12521362  149.77001953  151.9394989\n",
      "  152.64131165  153.09629822  152.90077209  152.71621704  152.66537476\n",
      "  152.65647888  148.22608948  150.16203308  150.94381714  150.29222107\n",
      "  151.64981079]\n",
      "iter:300, train_loss: -274585.375000, test_loss: -276586.000000\n",
      "train D prediction parameters results: [ 1745.85620117]\n",
      "test D prediction parameters results: [ 1746.10681152]\n",
      "train p_f prediction parameters results: [ 236.56036377  235.17346191  235.61857605  234.95254517  235.07385254\n",
      " -232.87176514  235.05267334  235.15432739  236.12123108 -234.36195374\n",
      "  233.92950439 -232.89389038  234.83752441  234.51078796  232.42312622\n",
      "  236.54399109  236.4259491   234.22680664  233.7130127   236.10787964\n",
      "  235.18838501 -231.65673828  236.31062317  234.22402954 -233.45314026\n",
      "  236.23300171  236.55860901  236.67507935  235.12849426  236.32719421\n",
      "  233.59931946  234.8135376   236.65119934  232.32302856  235.13252258\n",
      "  235.10794067  235.77059937 -234.41288757  235.41061401  231.61206055\n",
      "  235.17436218 -233.90707397 -229.651474    234.78366089  235.62786865\n",
      "  236.33178711 -234.64814758 -230.60375977  233.26055908  235.42831421\n",
      "  236.13044739  236.58584595  236.39085388  236.20526123  236.15359497\n",
      "  236.14587402  231.69958496  233.65377808  234.43481445  233.78601074\n",
      "  235.13600159]\n",
      "test p_f prediction parameters results: [ 236.56036377  235.17346191  235.61857605  234.95254517  235.07385254\n",
      " -232.87176514  235.05267334  235.15432739  236.12123108 -234.36195374\n",
      "  233.92950439 -232.89389038  234.83752441  234.51078796  232.42312622\n",
      "  236.54399109  236.4259491   234.22680664  233.7130127   236.10787964\n",
      "  235.18838501 -231.65673828  236.31062317  234.22402954 -233.45314026\n",
      "  236.23300171  236.55860901  236.67507935  235.12849426  236.32719421\n",
      "  233.59931946  234.8135376   236.65119934  232.32302856  235.13252258\n",
      "  235.10794067  235.77059937 -234.41288757  235.41061401  231.61206055\n",
      "  235.17436218 -233.90707397 -229.651474    234.78366089  235.62786865\n",
      "  236.33178711 -234.64814758 -230.60375977  233.26055908  235.42831421\n",
      "  236.13044739  236.58584595  236.39085388  236.20526123  236.15359497\n",
      "  236.14587402  231.69958496  233.65377808  234.43481445  233.78601074\n",
      "  235.13600159]\n",
      "iter:400, train_loss: -501934.531250, test_loss: -504529.031250\n",
      "train D prediction parameters results: [ 2358.63525391]\n",
      "test D prediction parameters results: [ 2358.55761719]\n",
      "train p_f prediction parameters results: [ 319.21212769  317.82263184  318.23080444  317.56311035  317.66531372\n",
      " -315.68865967  317.71520996  317.78048706  318.75094604 -317.04418945\n",
      "  316.57434082 -315.66244507  317.55206299  317.24493408  314.99783325\n",
      "  319.17468262  319.07745361  316.97113037  316.43148804  318.76113892\n",
      "  317.75250244 -314.55517578  318.96310425  316.79119873 -315.99105835\n",
      "  318.85015869  319.17706299  319.30633545  317.82147217  318.98080444\n",
      "  316.15496826  317.48675537  319.30407715  314.77255249  317.82025146\n",
      "  317.74832153  318.43249512 -317.13485718  318.03305054  314.06381226\n",
      "  317.81906128 -316.51916504 -312.26571655  317.40350342  318.25027466\n",
      "  318.9703064  -317.29476929 -313.30593872  315.91830444  318.0597229\n",
      "  318.75408936  319.24066162  319.05419922  318.83276367  318.74942017\n",
      "  318.81549072  313.73690796  316.2718811   316.98513794  316.63919067\n",
      "  317.71313477]\n",
      "test p_f prediction parameters results: [ 319.21212769  317.82263184  318.23080444  317.56311035  317.66531372\n",
      " -315.68865967  317.71520996  317.78048706  318.75094604 -317.04418945\n",
      "  316.57434082 -315.66244507  317.55206299  317.24493408  314.99783325\n",
      "  319.17468262  319.07745361  316.97113037  316.43148804  318.76113892\n",
      "  317.75250244 -314.55517578  318.96310425  316.79119873 -315.99105835\n",
      "  318.85015869  319.17706299  319.30633545  317.82147217  318.98080444\n",
      "  316.15496826  317.48675537  319.30407715  314.77255249  317.82025146\n",
      "  317.74832153  318.43249512 -317.13485718  318.03305054  314.06381226\n",
      "  317.81906128 -316.51916504 -312.26571655  317.40350342  318.25027466\n",
      "  318.9703064  -317.29476929 -313.30593872  315.91830444  318.0597229\n",
      "  318.75408936  319.24066162  319.05419922  318.83276367  318.74942017\n",
      "  318.81549072  313.73690796  316.2718811   316.98513794  316.63919067\n",
      "  317.71313477]\n"
     ]
    }
   ],
   "source": [
    "#load and save the parameters from last nn, \n",
    "\n",
    "\n",
    "#build nn\n",
    "\n",
    "\n",
    "#optimize the loss funciton by adding negative sign\n",
    "tf.reset_default_graph()\n",
    "p_f = tf.get_variable('p_f', shape=(1, 61)) #x2\n",
    "#p_f = tf.placeholder(shape=(None, 61), dtype=tf.float32) #x2\n",
    "#W2_1 = tf.get_variable('W2_1', shape=(61, 1))\n",
    "#W1_1 = tf.placeholder(shape=(18, 79), dtype=tf.float32)\n",
    "W1_1 = tf.constant(paras[0], dtype=tf.float32, name='W1_1' )\n",
    "#W1_2 = tf.placeholder(shape=(61, 79), dtype=tf.float32)\n",
    "W1_2 = tf.constant(paras[1], dtype=tf.float32, name='W1_2' )\n",
    "#b1 = tf.placeholder(shape=(1, 79), dtype=tf.float32)\n",
    "b1 = tf.constant(paras[2], dtype=tf.float32, name='b1' )\n",
    "#W2_1 =tf.placeholder(shape=(61, 1), dtype=tf.float32)\n",
    "W2_1 = tf.constant(paras[3], dtype=tf.float32, name='W2_1' )\n",
    "#W2_2 = tf.placeholder(shape=(79, 1), dtype=tf.float32)\n",
    "W2_2 = tf.constant(paras[4], dtype=tf.float32, name='W2_2' )\n",
    "#b2 = tf.placeholder(shape=(1, 1), dtype=tf.float32)\n",
    "b2 = tf.constant(paras[5], dtype=tf.float32, name='b2' )\n",
    "p_sub =  tf.placeholder(shape=(None, 18), dtype=tf.float32)\n",
    "#p_sub =  tf.get_variable('p_sub', shape=(1, 18))\n",
    "D = tf.matmul(tf.nn.sigmoid(tf.matmul(p_f, W1_2) + tf.matmul(p_sub, W1_1)+b1), W2_2)+tf.matmul(p_f, W2_1)+b2\n",
    "#print(D)\n",
    "\n",
    "p_loss = tf.reduce_mean(-p_f*D) #self defined loss\n",
    "\n",
    "###parameters\n",
    "#loss_new_price = tf.reduce_mean(losses)\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "gradients = trainer.compute_gradients(p_loss)\n",
    "optimizer = trainer.apply_gradients(gradients)\n",
    "\n",
    "# train\n",
    "#c_train = np.ones(( , 61)) #(bs, 61)\n",
    "#y_train = tv_train.iloc[step:step+batch_size, 2].values #(bs,)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #saver = tf.train.Saver([p_sub])\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(493):\n",
    "        #parameters \n",
    "        #x1_train = np.concatenate((tv_train.iloc[step:step+batch_size, 3:11].values, tv_train.iloc[step:step+batch_size, 14:67].values), 1)\n",
    "        x1_train = tv_train.iloc[step:step+batch_size, 66:84].values \n",
    "        x1_test = tv_test.iloc[step:step+batch_size, 66:84].values\n",
    "\n",
    "#         W1_1 = [l.tolist() for l in paras[0]]\n",
    "#         W1_2 = [l.tolist() for l in paras[1]]\n",
    "#         b1 = [l.tolist() for l in paras[2]]\n",
    "#         W2_1 = [l.tolist() for l in paras[3]]\n",
    "#         W2_2 = [l.tolist() for l in paras[4]]\n",
    "#         b2 = [l.tolist() for l in paras[5]]\n",
    "        \n",
    "        loss_tr, _ = sess.run([p_loss, optimizer], feed_dict={p_sub: x1_train})\n",
    "        _test_loss = sess.run([p_loss], feed_dict={p_sub: x1_test})\n",
    "        \n",
    "        # predict\n",
    "        D_pred_tr = sess.run([D], feed_dict={p_sub:x1_train})\n",
    "        D_pred_te = sess.run([D], feed_dict={p_sub:x1_test})\n",
    "        pf_pred_tr = sess.run([p_f], feed_dict={p_sub:x1_train})\n",
    "        pf_pred_te = sess.run([p_f], feed_dict={p_sub:x1_test})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"iter:%d, train_loss: %f, test_loss: %f\"%(step, loss_tr,  _test_loss[0]))\n",
    "            print('train D prediction parameters results:', D_pred_tr[0][0])\n",
    "            print('test D prediction parameters results:', D_pred_te[0][0])\n",
    "            print('train p_f prediction parameters results:', pf_pred_tr[0][0])\n",
    "            print('test p_f prediction parameters results:', pf_pred_te[0][0])\n",
    "        # save to disk ...\n",
    "        # saver...\n",
    "        \n",
    "        # predict change to predict / save Price!\n",
    "        #D_predict = sess.run([layer2_out], feed_dict={input1:X1_test,input2:X2_test})\n",
    "        \n",
    "        \n",
    "        ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try process into dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>SalesQuantity</th>\n",
       "      <th>SalesQuantityLag1</th>\n",
       "      <th>SalesQuantityLag7</th>\n",
       "      <th>SalesQuantityLag14</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "      <th>InventoryAvailability</th>\n",
       "      <th>WeekOfYear</th>\n",
       "      <th>...</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2626.270000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2651.693980</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2774.575000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2795.760000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2795.760000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2795.760000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2880.506000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-08</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2965.250000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-09</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2772.457500</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-10</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2824.009967</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID       Date  SalesQuantity  SalesQuantityLag1  SalesQuantityLag7  \\\n",
       "0  0 2016-01-01              2                  1                  1   \n",
       "1  0 2016-01-02              6                  2                  1   \n",
       "2  0 2016-01-03              5                  6                  1   \n",
       "3  0 2016-01-04              6                  5                  1   \n",
       "4  0 2016-01-05              2                  6                  1   \n",
       "5  0 2016-01-06              1                  2                  1   \n",
       "6  0 2016-01-07              6                  1                  1   \n",
       "7  0 2016-01-08              2                  6                  2   \n",
       "8  0 2016-01-09              5                  2                  6   \n",
       "9  0 2016-01-10              7                  5                  5   \n",
       "\n",
       "   SalesQuantityLag14        Price  Discount  InventoryAvailability  \\\n",
       "0                   1  2626.270000      1.01                   0.93   \n",
       "1                   1  2651.693980      1.01                   0.93   \n",
       "2                   1  2774.575000      1.01                   0.93   \n",
       "3                   1  2795.760000      1.01                   0.93   \n",
       "4                   1  2795.760000      1.01                   0.93   \n",
       "5                   1  2795.760000      1.01                   0.93   \n",
       "6                   1  2880.506000      1.01                   0.93   \n",
       "7                   1  2965.250000      1.01                   0.93   \n",
       "8                   1  2772.457500      1.01                   0.93   \n",
       "9                   1  2824.009967      1.01                   0.93   \n",
       "\n",
       "   WeekOfYear ...  7  8  9  0  1  2  3  4  5  6  \n",
       "0           1 ...  0  0  0  0  0  0  0  1  0  0  \n",
       "1           1 ...  0  0  0  0  0  0  0  0  1  0  \n",
       "2           1 ...  0  0  0  0  0  0  0  0  0  1  \n",
       "3           2 ...  0  0  0  1  0  0  0  0  0  0  \n",
       "4           2 ...  0  0  0  0  1  0  0  0  0  0  \n",
       "5           2 ...  0  0  0  0  0  1  0  0  0  0  \n",
       "6           2 ...  0  0  0  0  0  0  1  0  0  0  \n",
       "7           2 ...  0  0  0  0  0  0  0  1  0  0  \n",
       "8           2 ...  0  0  0  0  0  0  0  0  1  0  \n",
       "9           2 ...  0  0  0  0  0  0  0  0  0  1  \n",
       "\n",
       "[10 rows x 69 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try process into dummy\n",
    "tv_dummy=tv_info.copy()\n",
    "#process IDs \n",
    "Yfinal=tv_dummy.iloc[:,3]\n",
    "ID=[1138263,1139362,1139363,1141061,1142731,1143640,1144140,\n",
    "        1148001,1148010,1148081,1162466,1162467,1162557,1162558,\n",
    "        1162559,1163152,1163153,1164313,1164961,1164962,1165757,\n",
    "        1166153,1166984,1166998,1167021,1167087,1167847,1167918,\n",
    "        1170236,1170372,1170739,1173299,1174241,1174242,1174243,\n",
    "        1174244,1174275,1174293,1174299,1174313,1174314,1174315,\n",
    "        1174339,1174340,1175687,1175833,1175835,1175950,1177151]\n",
    "\n",
    "for i in range(len(ID)):\n",
    "    #print(tv_dummy['ID']==ID[i])\n",
    "    tv_dummy.loc[tv_dummy['ID']==ID[i], 'ID']=i\n",
    "    \n",
    "tv_dummy['ID'] = tv_dummy['ID'].astype(str)\n",
    "tv_dummy=pd.concat([tv_dummy, pd.get_dummies(tv_dummy.iloc[:, 0])], axis=1)     \n",
    "#print(tv_dummy.columns.tolist())\n",
    "#get dummy of weekday\n",
    "temp_date=tv_dummy['Date'].values\n",
    "tv_dummy['Date']=tv_dummy['Date'].dt.dayofweek\n",
    "tv_dummy['Date'] = tv_dummy['Date'].astype(str)\n",
    "#print(set(tv_dummy['Date'].values))\n",
    "tv_dummy=pd.concat([tv_dummy, pd.get_dummies(tv_dummy['Date'])],  axis=1)\n",
    "tv_dummy['Date']=temp_date\n",
    "\n",
    "tv_dummy.head(10)\n",
    "\n",
    "#get \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In progress: 0\n",
      "In progress: 500\n",
      "In progress: 1000\n",
      "In progress: 1500\n",
      "In progress: 2000\n",
      "In progress: 2500\n",
      "In progress: 3000\n",
      "In progress: 3500\n",
      "In progress: 4000\n",
      "In progress: 4500\n",
      "In progress: 5000\n",
      "In progress: 5500\n",
      "In progress: 6000\n",
      "In progress: 6500\n",
      "In progress: 7000\n",
      "In progress: 7500\n",
      "In progress: 8000\n",
      "In progress: 8500\n",
      "In progress: 9000\n",
      "In progress: 9500\n",
      "In progress: 10000\n",
      "In progress: 10500\n",
      "In progress: 11000\n",
      "In progress: 11500\n",
      "In progress: 12000\n",
      "In progress: 12500\n",
      "In progress: 13000\n",
      "In progress: 13500\n",
      "In progress: 14000\n",
      "In progress: 14500\n",
      "In progress: 15000\n",
      "In progress: 15500\n",
      "In progress: 16000\n",
      "In progress: 16500\n",
      "In progress: 17000\n",
      "In progress: 17500\n",
      "In progress: 18000\n",
      "In progress: 18500\n",
      "In progress: 19000\n",
      "In progress: 19500\n",
      "In progress: 20000\n",
      "In progress: 20500\n",
      "In progress: 21000\n",
      "In progress: 21500\n",
      "In progress: 22000\n",
      "In progress: 22500\n",
      "In progress: 23000\n",
      "In progress: 23500\n",
      "In progress: 24000\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6938e9697afe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0musim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtv_dummy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000000000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0musim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtv_dummy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1365\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1368\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1735\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1737\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1738\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1739\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Too many indexers'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m                 raise ValueError(\"Location based indexing can only have \"\n\u001b[0;32m    206\u001b[0m                                  \u001b[1;34m\"[{types}] types\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_has_valid_type\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1670\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1672\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_valid_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1673\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1674\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_valid_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_is_valid_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1711\u001b[0m         \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1712\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1713\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1714\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "#get corresponding top 9 substitute price and sale\n",
    "usim=tv_sim.copy()\n",
    "tv_dum_copy=tv_dummy.copy()\n",
    "tv_dummy = tv_dummy.assign(sim0=np.nan, sim1=np.nan, sim2=np.nan, sim3=np.nan, sim4=np.nan, \n",
    "                            sim5=np.nan, sim6=np.nan, sim7=np.nan, sim8=np.nan, sim9=np.nan, \n",
    "                            sim10=np.nan, sim11=np.nan, sim12=np.nan, sim13=np.nan, sim14=np.nan, \n",
    "                            sim15=np.nan, sim16=np.nan, sim17=np.nan)\n",
    "\n",
    "def find_idx(i):\n",
    "    idx = np.array(tv_dummy.index[tv_dummy.iloc[:,1]==tv_dummy.iloc[i,1]].tolist())\n",
    "    index = list(map(int, tv_dummy[tv_dummy.iloc[:,1]==tv_dummy.iloc[i,1]].ID.values))\n",
    "    diff = list(set(ID)-set(index))\n",
    "    return index, idx, diff\n",
    "\n",
    "ID=list(range(NumSKU))\n",
    "\n",
    "for i in range(len(tv_dummy)): #will take a long time!!! \n",
    "#for row in df.itertuples(): or try\n",
    "    if i%500==0: \n",
    "        print('In progress:', i)\n",
    "        \n",
    "    index, idx, diff=find_idx(i)\n",
    "    for di in diff:\n",
    "        usim.iloc[int(tv_dummy.iloc[i, 0]), di] = 10000000000\n",
    "    temp = usim.iloc[int(tv_dummy.iloc[i, 0]),:].values\n",
    "    M = sorted(temp)\n",
    "    I = np.argsort(temp)\n",
    "    for j in range(9):\n",
    "        try:\n",
    "            tv_dummy.iloc[i, 69+j*2: 69+j*2+2]=tv_dummy.iloc[idx[np.array(index==I[j])][0],6:8].values\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "tv_dummy.iloc[0:2, 69:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24697\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "descriptor 'toordinal' requires a 'datetime.date' object but received a 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-50f7fa13f8ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoordinal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mbinaries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: descriptor 'toordinal' requires a 'datetime.date' object but received a 'Series'"
     ]
    }
   ],
   "source": [
    "for i in range(len(info)):\n",
    "    usim=sim\n",
    "    row,col=np.where(info[:,2]==info[i,2])\n",
    "    index=[info[row,1], row]\n",
    "    diff=setdiff1d(ID,index[:,1])\n",
    "    usim[info[i,1],diff]=10000000000\n",
    "    for j in range(9):\n",
    "        M, I= msort(usim[info[i,1],:],'ascend')\n",
    "        r, c=np.where(index[:,1]==I[j])\n",
    "        info[i,NumVar+NumSKU+5+(j-1)*2:NumVar+NumSKU+5+(j-1)*2+1]=info[index(r,2),7:8]\n",
    "\n",
    "\n",
    "final=info[:,4:end]\n",
    "finalSens=final\n",
    "#     for i in range(3):\n",
    "#         finalSens{i}=(final)\n",
    "\n",
    "for i in range(5):\n",
    "    finalSens[1][:,64+(i-1)*2]=0.95*finalSens[1][:,64+(i-1)*2]\n",
    "    finalSens[2][:,64+(i-1)*2]=1.05*finalSens[2][:,64+(i-1)*2]\n",
    "\n",
    "mu=np.zeros(1,size(final,2))\n",
    "sd=np.zeros(1,size(final,2))\n",
    "                   \n",
    "for i in range(len(final)):\n",
    "    mu[1,i]=mean(finalSens[0][:,i])\n",
    "    sd[1,i]=std(finalSens[0][:,i])\n",
    "\n",
    " \n",
    "    for j in range(3):\n",
    "        for i in range(len(final)):\n",
    "            if np.std(1,i)>0:\n",
    "                finalSens[j][:,i]=(finalSens[j][:,i]-mu[1,i])/np.std(1,i)\n",
    "            if np.std(1,i)==0:\n",
    "                finalSens[j][:,i]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# full original script!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "24697\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must have equal len keys and value when setting with an ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-66307911fe12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mYfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#2 or 3?\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNumVar\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mNumVar\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mNumSKU\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNumSKU\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mmatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNumSKU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNumSKU\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNumSKU\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_has_valid_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    581\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m                         raise ValueError('Must have equal len keys and value '\n\u001b[0m\u001b[0;32m    584\u001b[0m                                          'when setting with an ndarray')\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Must have equal len keys and value when setting with an ndarray"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "\n",
    "info=tv_info.copy()\n",
    "\n",
    "for y in range(10):   #run ten times\n",
    "    #info = pd.read_excel('Television.xlsx', sheet_name='Main')\n",
    "    #sim = pd.read_excel('Television.xlsx', sheet_name='Similarity')\n",
    "    print(y)\n",
    "    Yfinal=info.iloc[:,3] #2 or 3?\n",
    "    print(len(info))\n",
    "    info.iloc[:,NumVar+1:NumVar+NumSKU-1]=np.zeros((len(info),NumSKU-1))\n",
    "    matrix = np.zeros((NumSKU, NumSKU))\n",
    "    for i in range(NumSKU):\n",
    "        sim[i,i]=1000000\n",
    "        \n",
    "    \n",
    "       \n",
    "    for i in range(len(info.iloc[:,0])):\n",
    "        for j in range(len(ID)):\n",
    "            if info.iloc[i,0]==ID[j]:\n",
    "                info.iloc[i,0]=j\n",
    "\n",
    "    ID=list(range(NumSKU))\n",
    "    \n",
    "    for i in range(len(info)):\n",
    "        for j in range(NumSKU-1): #dummy ids\n",
    "            if info.iloc[i,0]==j:\n",
    "                info[i,NumVar+j]=1\n",
    " \n",
    "    for i in range(len(info)):\n",
    "        d=date.toordinal(info.iloc[:,1])\n",
    "\n",
    "    binaries=np.zeros(len(info,1),6)\n",
    "    \n",
    "    for i in range(6):\n",
    "        for j in range(len(info)):\n",
    "            if d[j]==i:\n",
    "                binaries[j,i]=1\n",
    "            else:\n",
    "                binaries[j,i]=0\n",
    "#feature descriping wether is weekday   #dummy weekdays \n",
    "    info[:,NumVar+NumSKU-1:NumVar+NumSKU+4]=binaries\n",
    "    usim=tv_sim.copy()\n",
    "    \n",
    "    for i in range(len(info)):   #also dummying?   \n",
    "        row,col=np.where(info[:,2]==info[i,2])\n",
    "        index=[info[row,0], row]\n",
    "        diff=setdiff1d(ID,index[:,1])\n",
    "        usim[info[i,0],diff]=10000000000\n",
    "        for j in range(9):\n",
    "            M, I= msort(usim[info[i,0],:],'ascend')\n",
    "            r, c=np.where(index[:,1]==I[j])\n",
    "            info[i,NumVar+NumSKU+5+(j-1)*2:NumVar+NumSKU+5+(j-1)*2+1]=info[index(r,2),7:8]\n",
    "\n",
    "\n",
    "    final=info[:,4:end]  #select processed data from column 4\n",
    "    finalSens=final\n",
    "#     for i in range(3):\n",
    "#         finalSens{i}=(final)\n",
    "\n",
    "    for i in range(5):\n",
    "        finalSens[1][:,64+(i-1)*2]=0.95*finalSens[1][:,64+(i-1)*2]\n",
    "        finalSens[2][:,64+(i-1)*2]=1.05*finalSens[2][:,64+(i-1)*2]\n",
    "\n",
    "    mu=np.zeros(1,size(final,2))\n",
    "    sd=np.zeros(1,size(final,2))\n",
    "                   \n",
    "    for i in range(len(final)):\n",
    "        mu[1,i]=mean(finalSens[0][:,i])\n",
    "        sd[1,i]=std(finalSens[0][:,i])\n",
    "\n",
    " \n",
    "        for j in range(3):\n",
    "            for i in range(len(final)):\n",
    "                if np.std(1,i)>0:\n",
    "                    finalSens[j][:,i]=(finalSens[j][:,i]-mu[1,i])/np.std(1,i)\n",
    "                if np.std(1,i)==0:\n",
    "                    finalSens[j][:,i]=0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
